[["index.html", "CHAPTER 9: ANALYSIS OF HOUSEHOLD SURVEY DATA Abstract", " CHAPTER 9: ANALYSIS OF HOUSEHOLD SURVEY DATA Andrés Gutiérrez1, Pedro Luis do Nascimento Silva2 2024-11-21 Abstract Analyzing complex household survey data requires knowing and properly applying the foundations of the design-based inference. The researcher will be faced to a small database that contains specific information that will allow her to make conclusions over the whole population. The purpose of any analysis on this kind of datasets is not referred to make conclusions on the sample itself – which in most of the cases is a small subgroup of the population - but to the domains of interest and the whole population. Having that into account, the first step in any analysis plan should be devoted to defining the sampling design based on the selection mechanisms used to draw the final sample and the findings on the field related to nonresponse and lack of coverage. The chapter covers three main topics of analysis: descriptive statistics; comparisons and association; and modeling of survey data. On the one hand, we introduce simple descriptive statistics, such as totals, frequencies, means and proportions, quantiles and some graphics; on the other, we delve deeper on complex relationships between the variables of the survey. All these analyses rely on the representativity principle of the design-based inference. This way, the reader will find a strong focus, not only on point estimates, but also on uncertainty measures. The chapter also presents a short discussion on the different approaches that can be used to estimate variances; the best way to visualize the estimates; and NSO practical experiences. Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ SCIENCE, pedronsilva@gmail.com↩︎ "],["introduction.html", "Introduction", " Introduction A key concern for every agency that produces statistical information is ensuring the correct use of the data it provides. This concern is enshrined in the United Nations Fundamental Principles of Official Statistics, particularly in the following principles: Principle 3: To facilitate a correct interpretation of the data, statistical agencies must present information according to scientific standards, including details on the sources, methods, and procedures used. Principle 4: Statistical agencies are entitled to comment on erroneous interpretation and misuse of statistics. This chapter emphasizes empowering users to analyze household survey data accurately. The advent of the computer revolution, coupled with greater access to computational tools, has led to increased use of statistical data, including household survey data. Sometimes this data is used for purely descriptive purposes. Other times, however, its use is made for analytical purposes, involving the testing of hypothesis or the construction of models, when the objective is to draw conclusions that are also applicable to populations other than the one from which the sample was extracted. IHowever, when using standard statistical software for such analyses, results can be biased or misleading if the complex survey design is not properly accounted for. The primary focus of this chapter is to present relevant models, methods, and software that enable users to incorporate complex survey designs into their analyses effectively. What makes such data special for those who intend to use them for analytical purposes? Household survey data present unique challenges for analytical purposes because they are collected through complex sampling methods that often involve: Stratification: Dividing the population into comprehensive distinct subgroups before sampling. Clustering: Grouping sampling units to simplify data collection. Unequal probabilities of selection: Giving some sampling units a higher chance of being selected than others. Weighting adjustments: Correcting for non-response and/or improving precision. Standard data analysis methods and software ignore these features, leading to biased estimates of both the target parameters and their associated variances. In this chapter we analyze the impact of simplifications made when using standard data analysis methods and software, and present the necessary adjustments to these procedures in order to appropriately incorporate the aspects highlighted here into the analysis. In section 9.1, a short discussion on the fundamental principles of the design-based inference is presented, emphasizing that conclusions taken from probability sample surveys should be based on a pair: the point estimate and it associated margin of error (or any related measure). In section 9.2, we begin the journey with simple descriptive statistics: means, ratios, proportions and other typical descriptive parameters. Section 9.3 is devoted to more complex parameters that allow comparisons of the phenomenon of interest between subgroups for continuous and discrete variables. In this section we present standard tests to compare means and measure the degree of association between variables, and also consider the problem of correlation and association. Section 9.4 focuses on modelling survey outcomes. We first involve the reader in an discussion on the role of weighting when estimating regression coefficients. Then, we introduce some proper approaches to estimate complex parameters in linear and logistic regression models. Finally, section 9.5 presents a summary of ideas and tools for survey data visualization showing the best practices for creating graphics and maps in a context where uncertainty measures of estimates are important. The primary purpose of this chapter is to define and explain the fundamental concepts of the design-based paradigm in household surveys and demonstrate how to analyze complex household survey data. Throughout the chapter, national experiences are highlighted to illustrate how National Statistics Offices (NSOs) manage different stages of household survey data analysis. These practical examples provide a useful guide for applying the concepts and methods discussed in real-world contexts. By the end of the chapter, readers will be equipped with the knowledge and tools needed to analyze household survey data effectively while accounting for the complexities of survey design. "],["planning-analysis.html", "1 Planning Analysis", " 1 Planning Analysis Planning the analysis stage of a survey is an essential part of the overall survey planning process. Following the Generic Statistical Business Process Model (GSBPM) (https://unece.org/statistics/documents/2019/01/standards/gsbpm-v51), this step corresponds to the subprocess labeled as 2.1 - Design Outputs. At this stage, it is important to distinguish between two groups of analysts: primary data producers and secondary data users. Proper planning and understanding of the survey design are crucial for both primary data producers and secondary data users. For primary data producers, creating a comprehensive tabular plan ensures alignment with survey objectives. For secondary users, clear research questions and attention to survey metadata enable accurate and meaningful analyses. "],["primary-data-producers.html", "1.1 Primary data producers", " 1.1 Primary data producers They are responsible for planning and executing the survey to collect the intended data. For them, planning the analysis typically involves preparing a tabular plan—a document specifying the core set of tables to be produced once the survey data becomes available. This plan ensures that the survey results align with the stated objectives of the survey. Primary data producers are also responsible for defining the survey tabular plan, which is a document describing which tables are to be produced once the data are available. This must align well with the needs specified for the survey (see chapter 2). Preparing this plan generally requires defining three sets of specifications: Filter Conditions: These may be used to define subgroups of the population for which specific tables will be produced. For example, in a survey where questions regarding occupation are asked only from individuals aged 15 or older, a filter condition might be ‘if age &gt; 14’. Such a condition would then mean that only those in the relevant age group would be included in tables for the occupation related variables (status, type, income, etc.). Classification or Domain Variables: These are variables used to subdivide the population into meaningful groups for analysis. For example, geographic areas (e.g., states or provinces), age groups, or sex might define rows in a table. These variables are often chosen to meet reporting requirements, such as providing estimates by province in national household surveys. A typical list of domain defining variables would include: geographic levels (provinces, etc.); type of region (urban x rural); sex; age groups; education; race / etchnicity; etc. Response or Survey Variables: These are the variables being analyzed to understand how they vary across the defined domains. For instance, continuous survey variables (like income) might be used to create columns in a table, summarizing means, medians, or other statistics. These variables are all others that we do not use as classification ones. Categorical survey variables (like employment status) will generate a column for each category where the corresponding cross-classified frequencies will appear, with one row for each of the classes of the domain defining variables. An important consideration when defining domains has to do with sample design, namely when defining strata and sample sizes. For most national household sample surveys, providing breakdown by province or state is required, and therefore stratification by provinces will be essential. Also, if precision requirements must be satisfied at the province level, then sample sizes that meet such requirements must be computed by province and then summed to obtain the country level sample size. For domains defined by characteristics that are unavailable from the sampling frame, say age groups, for the case of household surveys that use area sampling, sample size calculations must take into account what the required sample size must be such that estimates for the rarest group meet precision specifications. As an example, suppose that estimates are required by age groups such as young adults (18 to 29), adults (30 to 49), ageing adults (50 to 59) and elderly (60 and over). Assuming that the population distribution by these age groups is such that the ageing adults is the rarest group with 12.5% of the total population, and if a minimum sample size of 500 individuals in this group is required, then the total sample must be at least 500 / 0.125 = 4,000. That is, in order for the full sample to provide an expected sample of about 500 ageing adults we must sample at least 4,000 individuals for the survey. "],["secondary-data-users.html", "1.2 Secondary Data Users", " 1.2 Secondary Data Users Secondary data users are all those who will access and analyze the survey data after it has been released, typically with access to only the public datasets and documentation provided by the data producers. Their primary task is to define clear research questions and locate relevant documentation. Therefore, a key step will be the location and understanding of the survey metadata, which must describe the sampling design and estimation methods used (including details about stratification, clustering, and survey weights), both for descriptive parameters and for the corresponding measures of precision. For example, based on the aforementioned, a well-defined research question posed by some secondary data user, can be such as “Do rural households face digital exclusion compared to urban households?”. Notice that it directs the analysis towards estimating relevant parameters and precision measures. The question might involve testing whether the proportion of rural households with internet access is significantly lower than that of urban households. This clarity allows for more direct and accurate analyses. The null hypothesis behind this question is that \\(H_0) P_{Rural} = P_{Urban}\\), which we wish to test against the alternative \\(H_A) P_{Rural} &lt; P_{Urban}\\), where \\(P\\) denotes the proportion of households having internet access in the domain identified. A related but different set of hypothesis could be \\(H_0) C_{Rural} = C_{Urban}\\) versus \\(H_A) C_{Rural} &lt; C_{Urban}\\) where \\(C\\) denotes the average cost of connection for internet access in the specified domain. Listing the research questions in this way would enable the subsequent analysis to progress more directly towards the estimation of the relevant parameters from the survey data, and corresponding precision measures, both of which are required to compute test statistics that would provide the evidence required to answer them. But in order for such estimation to take place, the secondary data user will first have to find out details about how the sampling and estimation were done for the particular survey at hand. As we will discuss in subsequent sections of this chapter, it is essential to account for the survey weights when computing point estimates of both descriptive or model parameters, and to account for structural components of the sampling design and estimation process (stratification, clustering, unequal inclusion probabilities, non-response adjustment and calibration of survey weights, if any) when estimating variances or other measures of precision of the point estimates. Users that disregard such aspects of the sampling design do so at their peril, and may end up producing biased estimates that will lead to incorrect inferences and decisions. The recommended practice is for data producers to provide sufficient detail about such sampling aspects as part of the metadata released with survey microdata, in order to enable secondary data users to consider these aspects when conducting their analyses of interest. 1.2.1 Quality Control for Secondary Analysts A standard quality control step for secondary data analysts would consist of the following steps: Load the Data and Metadata: Ensure that the survey microdata is properly linked to metadata describing the sampling design and estimation processes. Replicate Published Estimates: Recreate some of the estimates provided by the survey producers, including measures of precision, to confirm that the data and design have been correctly interpreted. Compare Scenarios: When the analyst is capable of replicating published estimates, can then proceed towards the required new analysis for which no previous results are available. Repeat this analysis under two scenarios: ignoring and accounting for the sampling design. Compare the results and assess the impact of incorporating the sample design. Finalize the Analysis: Use only the results that account for the sampling design in the final interpretation, ensuring that the design effects are appropriately incorporated. "],["accounting-for-the-sampling-design.html", "2 Accounting for the sampling design", " 2 Accounting for the sampling design When analysis household survey data, ignoring the sampling design undermines the representativeness, accuracy, and credibility of survey-based findings, which can lead to incorrect decisions. This is why accounting for the sampling design is essential when analyzing household survey data to ensure valid and unbiased estimates. As seen in the previous chapters, regular household surveys has two major characteristics: They use complex sampling designs (e.g., stratification, clustering, and unequal probabilities of selection) to represent the population efficiently. Ignoring the design can lead to biased population-level inferences. They define sampling weights for each sampling unit (primary, and remaining ones) to properly represent the population. To illustrate this fact, we provide a simple example. Suppose a country has two regions: Region A with 100 people, and Region B with 900 people. Wealthy people lives in Region A, with an average income of $10,000, less wealthy people lives in Region B, with average income of $2,000. The true population mean income is $2,800, because: \\[ \\theta = \\frac{(100 \\times 10,000) + (900 \\times 2,000)}{100 + 900} = 2,800. \\] Suppose a survey is conducted where 50 people are sampled from each Region. After the collection of data, it was found that the sample mean for Region A was $10,000, while the sample mean for Region B was $2,000. If the sampling design is ignored, all units in the sample will receive equal weights, regardless of their population sizes. This way, the mean income is biasedly estimated by: \\[ \\hat \\theta = \\frac{(50 \\times 10,000) + (50 \\times 2,000)}{100} = 6,000. \\] When the sampling design is considered, weights are applied proportional to the population sizes of each neighborhood. This way, units in Region A will receive a weight of \\(\\frac{100}{50} = 2\\), while units in Region B will receive a weight of \\(\\frac{900}{50} = 18\\). In this escnario, the mean income is unbiasedly estimated by: \\[ \\hat \\theta = \\frac{(2 \\times 50 \\times 10,000) + (18 \\times 50 \\times 2,000)}{(2 \\times 50) + (18 \\times 50)} = 2,800. \\] Ignoring the sampling design causes that Region A (smaller but wealthier) dominates the estimate, even though it represents only 10% of the country. This creates a bias, making the income seem higher than it actually is for the whole population. By considering the sampling design and using weights, and strata, the estimate correctly reflects the true contribution of each Region, avoiding bias. In real applications, to account for the sampling design, we must ensure that primary sampling units (PSU), strata, and weights are available in the survey dataset to enable adequate analysis. Alternatively, when such information is not available, the dataset should at least contain replicate weights, or the analyst should have clear guidance on how to compute both point and variance estimates. This section discusses how survey data from a sample can be used to draw conclusions about an entire population using a design-based approach. This method assumes that the sample is selected through a well-defined probability sampling process, which ensures that every unit in the population has a known, non-zero chance of being included. Sampling weights, which reflect how much each sampled unit represents in the population, are essential tools in this approach. These weights allow analysts to make estimates that account for the sampling process and produce results that are representative of the population. A well-described survey design facilitates statistical analysis, supports effective data interpretation, and enables meaningful insights into complex phenomena. Not accounting for it may lead to biased estimates and misleading conclusions. "],["parameters-and-estimators.html", "2.1 Parameters and estimators", " 2.1 Parameters and estimators Two common goals when analyzing survey data are to estimate the total value of a characteristic for the population, such as total income, and to calculate the average value of that characteristic, such as the average income per person. These are referred to as the population total and the population mean, respectively. The design-based approach accounts for the inclusion of the probability sampling design in the inferential process. That is, it is assumed that every unit in the population has a known chance of being included in the sample. These probabilities are used to calculate the sampling weights. When the design is properly implemented, the estimates for the population totals and means are unbiased. This means that, on average, the estimates will equal the true population values if the survey were to be repeated many times under the same conditions. Under this approach, the estimators used to make inferences about the parameters of the population use the sampling weights to create weighted sums of the survey data, which serve as estimates for these population values. If the weights are appropriately applied, the resulting estimates are consistent with the true values in the population. As stated in the previous chapter, in some situations, the original sampling weights need adjustments to improve their accuracy. Adjustment may accoutn for survey non-response, where weights are corrected to account for individuals who were selected for the survey but did not participate. These adjusted weights help to minimize biases in the estimates and make the results more reliable. If calibration was performed, the weights are modified to ensure that the weighted sums aligns more closely with known characteristics of the population, such as age or gender distributions. The population total \\(Y = \\sum _{U} y_k\\) and mean \\(\\overline Y = \\frac Y N\\) of a survey variable \\(y\\) can be estimated by weighted estimators given by \\(\\widehat Y _{HT} = \\sum _{s} d_k \\ y_k\\) and \\(\\overline y_{H} = \\frac {\\widehat Y_{HT}} {\\widehat N_{HT}} = \\frac {\\sum_{s} d_k \\ y_k} {\\sum_{s} d_k}\\), respectively. When the survey weights are calibrated and/or non-response adjusted, the above expressions may still be used, but with the calibrated or non-response adjusted weights, \\(w_k\\) say, replacing the design weights \\(d_k\\), for all \\(k \\in s\\). Here \\(s = \\left\\{ k_1, \\ldots, k_n \\right\\} \\subset U\\) denotes the set of units in a sample selected from the population \\(U\\) using a probability sampling design \\(p(s)\\), that ensures strictly positive first order inclusion probabilities \\(\\pi_k = Pr(k \\in s), \\, \\forall \\, k \\in U\\). These inclusion probabilities are assumed known \\(\\forall \\, k \\in s\\), at least to the data producers. An important part of survey analysis is understanding the level of uncertainty in the estimates. Since we are working with a sample and not the entire population, there will always be some variability in the results. This variability is measured using the sampling variance, which indicates how much the estimate might differ from the true population value in repeated samples. While there are theoretical formulas to calculate this variance, these can be complex and rely on information that is not always available to analysts. Under the design-based framework and assuming full response, \\(\\widehat Y _{HT}\\) is unbiased for \\(Y\\) and its sampling variance can be estimated unbiasedly by \\[ \\widehat {V_p} \\left( \\widehat{Y}_{HT} \\right) = \\sum_{k \\in s} \\sum_{j \\in s} \\left( d_k d_j - d_{kj} \\right) y_k y_j \\] Where \\(d_{kj} = 1 / \\pi_{kj}\\) and \\(\\pi_{kj} = Pr(k,j \\in s), \\, \\forall \\, k,j \\in U\\). This result assumes that the sampling design \\(p(s)\\) is such that \\(\\pi_{kj} &gt; 0 \\,\\, \\forall \\, k,j \\in U\\). While the above formula for variance estimation is general and covers the vast majority of sample designs used in the practice of household sample surveys, it is not used in practice because the second order inclusion probabilities \\(\\pi_{kj}\\) (and corresponding pairwise weights \\(d_{kj}\\)) are generally unknown to survey data analysts. In fact, even data producers do not compute such pairwise weights, since there are more efficient methods for variance estimation that do not require having such weights. For this reason, simpler and more efficient methods are often used in practice, allowing analysts to quantify the uncertainty without requiring overly detailed information about the sampling design. "],["approaches-to-variance-estimation.html", "2.2 Approaches to Variance Estimation", " 2.2 Approaches to Variance Estimation When working with household surveys, the sample is usually only a small subset of the entire population. Because of this, it is important to measure not only the main estimates of interest, such as totals or averages, but also the level of uncertainty in these estimates. This uncertainty is often expressed as measures of variance or confidence intervals, which help us understand how much the estimates might differ if the survey were repeated. Understanding and estimating uncertainty is a critical part of analyzing household survey data; by using proper methods, analysts can measure the reliability of their estimates. There are several methods to estimate the uncertainty in survey results: One common approach is based on approximate formulas like Taylor linearization, which simplifies complex relationships between variables into linear ones. The ultimate cluster method, is used in surveys that collect data through multi-stage sampling, where groups of people (clusters) are selected at different stages, exactly as household survey data is collected. Generalized variance functions (David A. Binder 1983b), which comprises a unifying idea of sampling theory. Estimating equations provides a flexible framework for calculating totals, means, ratios, and other parameters from both the population and the sample. 2.2.1 Estimating equations With the help of modern software, these methods can be implemented efficiently, ensuring accurate and meaningful analysis of survey data. Many population parameters can be written/obtained as solutions for population estimating equations. Variance estimation for these sample-based methods follows a consistent framework. Although the details can be technical, the key idea is that the same principles used to estimate totals can be applied to calculate variances. This generality makes the method simple and versatile, allowing it to be implemented in widely used software like the R survey package and the Stata svy functions. These tools automate much of the process, making it accessible for analysts to estimate both population parameters and their associated uncertainties. A generic population estimating equation is given by $_{i U} z_i () = 0 $, where \\(z_i(\\bullet)\\) is an estimating function evaluated for unit \\(i\\) and \\(\\theta\\) is a population parameter of interest. These equations provide a general way to describe and calculate many population parameters, such as totals, means, and ratios. The concept is straightforward: population parameters can be defined as solutions to specific equations that involve all the units in the population. This approach is flexible and can be adapted to calculate many different types of parameters. For the case of the population total, take \\(z_i(\\theta) = y_i - \\theta / N\\). The corresponding population estimation equation is given by \\(\\sum _{i \\in U} (y_i - \\theta / N) = 0\\), and solving for \\(\\theta\\) gives the population total \\(\\theta_U = \\sum _{i \\in U} y_i \\ = \\ Y\\). For ratios of population totals, taking \\(z_i(\\theta) = y_i - \\theta x_i\\), the corresponding population estimation equation is given by \\(\\sum _{i \\in U} (y_i - \\theta x_i) = 0\\). Solving for \\(\\theta\\) gives the population ratio \\(\\theta_U = \\sum _{i \\in U} y_i / \\sum _{i \\in U} x_i \\ = \\ R\\). Similarly, for means, take \\(z_i(\\theta) = y_i - \\theta\\). The idea of defining population parameters as solutions to population estimating equations allows defining a general method for obtaining corresponding sample estimators. It is a matter of using the sample estimating equations \\(\\sum _{k \\in s} d_k \\, z_k (\\theta) = 0\\). Under probability sampling, full response and with \\(d_k = 1 / \\pi_k\\), the sample sum in the left hand side is unbiased towards the population sum in the corresponding population estimating equation. Solving the sample estimating equation yields consistent estimators for the corresponding population parameters. A consistent estimator for the variance of estimators obtained as solutions of sample estimating equations can be obtained as: \\[ \\widehat V_p (\\widehat \\theta) = \\left[ \\widehat J ( \\widehat \\theta) \\right] ^{-1} \\widehat V_p \\left[ \\sum _{k \\in s} d_k \\, z_k (\\widehat \\theta)\\right] \\left[ \\widehat J ( \\widehat \\theta) \\right] ^{-1} \\] Where \\(\\widehat J (\\widehat \\theta) = \\sum _{k \\in s} d_i \\, \\left[ \\partial{z_k ( \\theta)} / \\partial \\theta \\right]_{\\theta = \\widehat \\theta}\\). This approach implies that by one is able to estimate many population parameters and corresponding variances using essentially well known methods for estimating totals. Its simplicity and generality have enabled the development of software such as the R survey package, the Stata svy functions and others. 2.2.2 Ultimate Cluster Method The Ultimate Cluster method is a straightforward and powerful approach for estimating the uncertainty (variance) of totals in surveys that use multi-stage cluster sampling designs. This method, proposed by Hansen, Hurwitz, and Madow (1953), and simplifies the complex nature of multi-stage designs by focusing only on the variation between the largest groups, known as Primary Sampling Units (PSUs). It assumes that the PSUs were selected randomly and independently, even if they were not actually chosen this way in the sampling process. This assumption allows for a simpler analysis while still providing reliable variance estimates. The method considers only the variation between information available in the level of PSUs, and assumes that these would have been selected with replacement from the PSU population. This idea is simple, but quite powerful, because it allows to accommodate a variety of sampling designs, involving stratification and selection with unequal probabilities (with or without replacement) of both PSUs as well as lower level sampling units (households and individuals). The requirements for the application of this method are: One has unbiased estimators of totals for the variable of interest for each sampled PSU. Data are available for at least two sampled PSUs in each stratum (if the sample is stratified in the first stage). The survey database contains all the information regarding PSUs, strata and weights. Consider a multi-stage sampling design, in which \\(m_{h}\\) PSUs are selected in stratum \\(h,\\) \\(h=1,\\ldots ,H\\). Let \\(\\pi_{hi}\\) be the inclusion probability of PSU \\(i\\) stratum \\(h\\), and by \\(\\widehat{Y}_{hi}\\) an unbiased estimator of the total \\(Y_{hi}\\) of the survey variable \\(y\\) for the \\(i\\)-th PSU in stratum \\(h\\), \\(h=1,\\ldots ,H\\). Hence an unbiased estimator of the population total \\(Y = \\sum_{h=1}^{H} \\sum_{i \\in U_{1h}} Y_{hi}\\) is given by \\(\\widehat{Y}_{UC} = \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} d_{hi} \\widehat{Y}_{hi}\\), and the ultimate cluster estimator of the corresponding variance is given by: \\[ \\widehat{V}_{UC} \\left( \\widehat{Y}_{UC}\\right) = \\sum_{h=1}^{H} \\frac{m_{h}} {m_{h}-1} \\sum_{i=1}^{n_{h}} \\left( d_{hi} \\widehat{Y}_{hi} - \\frac{\\widehat{Y}_{h}}{m_{h}} \\right) ^{2} \\] where \\(U_{1h}\\) and \\(s_{1h}\\) are the population and sample sets of PSUs in stratum \\(h\\), \\(d_{hi} = 1 / \\pi_{hi}\\), \\(\\widehat{Y}_{h} = \\sum_{i=1}^{n_{h}} d_{hi} \\widehat{Y}_{hi}\\) for \\(h=1,\\ldots ,H\\). (See for example, (Babubhai. V. Shah et al. 1993), p. 4). Although the method was originally proposed for estimation of variances of estimated totals, it can also be applied in combination with estimating equations to obtain variance estimates for estimators of other population quantities that can be obtained as solutions to sample estimating equations. This makes the method versatile and useful for a wide range of applications in survey analysis. One key assumption of the method is that the PSUs were selected independently and with replacement. In reality, many surveys select PSUs without replacement, which is a more efficient design. However, the variance estimates produced by the Ultimate Cluster method are generally close enough to be useful, even under these conditions. This practical simplicity is why the method is widely used in survey analysis. The Ultimate Cluster method is particularly attractive because of its simplicity. Survey practitioners often prefer it over more complex approaches that account for all stages of the sampling design. Although these detailed methods may provide slightly more accurate variance estimates, they are significantly harder to implement and require more detailed information about the sampling process. In contrast, this method offers a reasonable approximation that works well for most practical purposes, especially for estimating totals or averages. A discussion about Quality of this approximation and alternatives can be found in (Särndal, Swensson, and Wretman 1992), p. 153. 2.2.3 Bootstrap Method When the user does not have access to information of PSUs or strata in the database, the Ultimate Cluster method cannot be used, and some other methods should be considered. Among them, we have replication-based methods; in particular, the bootstrap method, which comprises a powerful and flexible approach for estimating variances in surveys, particularly when dealing with complex survey designs that involve multiple stages or stratification. Originally proposed by Efron (1979), the version commonly used for household surveys is called the Rao-Wu-Yue Rescaling Bootstrap (Rao, Wu, and Yue 1992) (XXXX reference). This method is well-suited for stratified multi-stage sampling designs and has become a widely used tool for analyzing complex survey data. The bootstrap method relies on creating many new “replicated” datasets, which are slightly different versions of the original sample. These replicated datasets mimic the process of repeatedly drawing samples from the population. By analyzing the variation in results across these datasets, we can estimate how much uncertainty there is in our estimates from the original sample. First, we create a new sample for each stratum by randomly selecting primary sampling units (PSUs) from the original sample, allowing PSUs to be selected more than once (with replacement). Each selected PSU is included in the new dataset along with all its associated data. The size of this random sample with replacement is of \\(m_h - 1\\) PSUs in each of the \\(H\\) design strata. This process of creating new samples is repeated many times, usually hundreds or thousands, to produce multiple “replicated” datasets. That is, repeat Step 1 \\(R\\) times, and denote by \\(m_{hi}(r)\\) the number of times the PSU \\(i\\) of stratum \\(h\\) was selected for the sample in replicate \\(r\\). For each replicate, bootstrap weights are calculated for each unit. These weights account for how often each PSU appears in the replicate and ensure that the replicated datasets remain representative of the population. The bootstrap weight of unit \\(k\\) within PSU \\(i\\) of stratum \\(h\\) is \\(w_{hik} (r) = w_{hik} \\times \\frac {m_h}{m_h - 1} \\times m_{hi}(r)\\). The parameter of interest, such as a total or mean, is estimated for each replicated dataset using the bootstrap weights. That is, for each replica \\(r\\), calculate an estimate \\(\\widehat \\theta_{(r)}\\) of the target parameter \\(theta\\) using the bootstrap weights \\(w_{hik} (r)\\). Finally, the variability of the results across all replicated datasets is used to estimate the variance. The idea is that the variation in these replicate estimates reflects the uncertainty in the original estimate. This estimate of the variance takes the following form: \\[ \\widehat V_{B} \\left( \\widehat \\theta \\right) = \\frac {1} {R} \\sum_{r=1}^R \\left( \\widehat \\theta_{(r)} - \\tilde \\theta \\right)^2 \\] where \\(\\tilde \\theta = \\frac 1 R \\sum_{r=1}^R \\widehat \\theta_{(r)}\\) is the average of the replica estimates. Whenever the original sampling weights \\(w_{hik}\\) receive non-response adjustments or are calibrated, the corresponding non-response adjustments and/or calibration of the basic weights must be repeated for each replica, so that the variance estimates adequately reflect the effects of the calibration and non-response adjusments on the uncertainty of the point estimates. This ensures that the variance estimates accurately reflect the additional uncertainty introduced by these adjustments. The bootstrap method has several advantages. It works well for complex survey designs and can handle a wide range of parameters, including those that are difficult to estimate using traditional methods, such as medians or other nonlinear statistics. It also provides a way to estimate variances when other methods are not available or practical to use. The method is particularly helpful for survey analysts who may not have access to specialized software for calculating variances. Many modern statistical software tools, including the survey package in R, support bootstrap replication and variance estimation, making it accessible to a wide range of users. While the bootstrap method is computationally intensive, requiring many replicated datasets to be created and analyzed, it is highly effective. It provides robust variance estimates even for complex parameters and remains one of the most flexible tools for analyzing survey data. References Binder, David A. 1983b. “On the Variances of Asymptotically Normal Estimators from Complex Surveys.” International Statistical Review 51 (3): 279–92. https://doi.org/10.2307/1402588. Efron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. Hansen, Morris H., William N. Hurwitz, and William G. Madow. 1953. Sample Survey Methods and Theory. Vol. 1 and 2. New York: John Wiley; Sons. Rao, J. N. K., C F J Wu, and K. Yue. 1992. “Some Recent Work on Resampling Methods for Complex Surveys.” Survey Methodology 18: 209–17. Särndal, Carl-Erik, Bengt Swensson, and Jan Wretman. 1992. Model Assisted Survey Sampling. New York: Springer-Verlag. Shah, Babubhai. V, Ralph E Folsom, Lisa LaVange, Sara C Wheeless, Kerrie E Boyle, and Rick L Williams. 1993. “Statistical Methods and Mathematical Algorithms Used in SUDAAN.” Research Triangle Institute. "],["using-software-to-generate-valid-inferences.html", "2.3 Using software to generate valid inferences", " 2.3 Using software to generate valid inferences The design and analysis of information from household surveys must include extensive use of existing computational tools. This section reviews in detail the computational approaches of the statistical software used for each of the statistical processes required to publish official figures with high levels of accuracy and reliability. Specifically, for the following processes: Sample selection according to the defined sampling design. Generation of sampling weights for each individual and household. Modeling of nonresponse and statistical imputation. Calibration of sampling weights and adjustments for nonresponse. Estimation of sampling errors for each indicator of interest in the statistical production tables. Analysis of multivariate relationships between survey variables. Nations (2005, sec. 7.8) highlights the importance of including the structure of complex survey designs in the inference process for estimating official statistics from household surveys. It warns, with an empirical example, that failing to do so may result in biased estimates and underestimated sampling errors. Below are some key features that statistical software packages incorporate when managing data from complex survey designs, such as those found in household surveys. A more detailed review, including syntax and computational code, can be found in Steven G. Heeringa, West, and Berglund (2017a, Appendix A). In general, these computational tools are designed to enhance the efficiency of variance approximation methods for complex samples, as well as replication techniques to estimate design-based variances (Westat 2007). Some of these software packages are free to use, although most are licensed products requiring paid licenses. These products, in addition to providing descriptive statistics (such as means, totals, proportions, percentiles, and ratios), allow for fitting linear and logistic regression models. All resulting statistics are based on the survey design. R R is a free software increasingly used in social research, as it is likely to host the latest scientific findings implemented in this software (R Core Team 2024). Being open-source, researchers can upload their own collections of computational functions to the official repository (CRAN) and make them available to the community. The samplesize4surveys package (Rojas 2020) determines the sample size for individuals and households in repeated, panel, and rotational household surveys. The sampling (Tillé and Matei 2016) and TeachingSampling (Hugo Andrés Gutiérrez 2015) packages enable the selection of probabilistic samples from sampling frames under a wide variety of designs and algorithms. The survey package (Lumley 2016), once the survey design is predefined using the svydesign() function, allows for analyzing household survey data and obtaining appropriate standard error estimates. STATA The svy environment provides tools for appropriate inference of official statistics from household surveys (STATA 2017). The svyset command specifies variables identifying survey design features, such as sampling weights, clusters, and strata. The svydescribe command produces tables describing strata and sampling units at a given survey stage. Once survey design definitions are loaded, any model can be estimated, and the resulting statistics will be survey-design-based. The svy environment also supports predictive commands. SPSS The complex samples module in SPSS (IBM 2017) supports the selection of complex samples through user-defined sampling schemes. Next, an analysis plan must be created by assigning design variables, estimation methods, and sample unit sizes. Once the sampling plan is defined, the module enables the estimation of counts, descriptive statistics, and crosstabulations. It is also possible to estimate ratios and regression coefficients in linear models, along with corresponding hypothesis test statistics. Finally, the module allows for estimating nonlinear models, such as logistic regressions, ordinal regressions, or Cox regressions. SAS This statistical software includes a procedure for selecting probabilistic samples called SURVEYSELECT, which integrates common selection methods such as simple random sampling, systematic sampling, probability proportional to size sampling, and stratified allocation tools. To analyze data from complex samples, specific procedures have been programmed (SAS 2010): SURVEYMEANS: Estimates totals, means, proportions, and percentiles, along with their respective standard errors, confidence intervals, and hypothesis tests. SURVEYFREQ: Estimates descriptive statistics (e.g., totals and proportions) in one- and two-way tables, provides sampling error estimates, and analyzes goodness-of-fit, independence, risks, and odds ratios. SURVEYREG and SURVEYLOGISTIC: Fit linear and logistic regression models, respectively, estimating regression coefficients with associated errors and providing an exhaustive analysis of model properties. SURVEYPHREG: Fits survival models using pseudo-maximum likelihood techniques. References Gutiérrez, Hugo Andrés. 2015. TeachingSampling: Selection of Samples and Parameter Estimation in Finite Population. https://CRAN.R-project.org/package=TeachingSampling. Heeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2017a. Applied Survey Data Analysis. Chapman and Hall CRC Statistics in the Social and Behavioral Sciences Series. CRC Press. IBM. 2017. IBM SPSS Complex Samples. ftp://public.dhe.ibm.com/software/analytics/spss/documentation/statistics/23.0/en/client/Manuals/IBM_SPSS_Complex_Samples.pdf. ———. 2016. “Survey: Analysis of Complex Survey Samples.” Nations, United. 2005. Household Surveys in Developing and Transition Countries. New York, NY: United Nations. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rojas, Hugo Andres Gutierrez. 2020. Samplesize4surveys: Sample Size Calculations for Complex Surveys. SAS. 2010. SAS/STAT 9.22 User’s Guide - Survey Sampling and Analysis Procedures. https://support.sas.com/documentation/cdl/en/statugsurveysamp/63778/PDF/default/statugsurveysamp.pdf. STATA. 2017. STATA Survey Data. https://www.stata.com/manuals13/svy.pdf. Tillé, Yves, and Alina Matei. 2016. Sampling: Survey Sampling. https://CRAN.R-project.org/package=sampling. Westat. 2007. WesVar 4.3. Users Guide. http://users.nber.org/~jroth/chap1.pdf. "],["descriptive-parameters.html", "3 Descriptive parameters", " 3 Descriptive parameters The most frequent analysis of complex household survey data consists in estimating some descriptive population parameters for a range of survey variables. Such descriptive analysis generally involve estimating frequencies, proportions, means, and totals. But other target parameters such as selected quantiles of numeric variables, poverty and inequality measures, and a range of indicators such as those required for monitoring the Sustainable Development Goals are becoming part of regular set of estimates needed from household sample surveys. "],["frequencies.html", "3.1 Frequencies", " 3.1 Frequencies 3.1.1 Point Estimation The accurate estimation of absolute sizes and proportions in household surveys is essential for obtaining representative data that reflects the demographic and socioeconomic reality of a population. These figures serve as the basis for public policy decision-making, resource allocation, and the design of social programs. The ability to understand the distribution of specific categories, such as poverty status, employment status, education level, among others, provides valuable information to address inequalities and promote equitable development. 3.1.2 Size Estimates In this section, the processes for estimating categorical variables will be carried out. First, one of the most important parameters is the size of a population, which represents the cardinality of that set; in other words, the total number of individuals that comprise it. In terms of notation, the population size is estimated as follows: \\[ \\widehat{N} = \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} \\] where \\(s_{hi}\\) is the sample of households or individuals in PSU \\(i\\) of stratum \\(h\\); \\(s_{1h}\\) is the sample of PSUs within stratum \\(h\\); and \\(w_{hik}\\) is the weight (expansion factor) of unit \\(k\\) within PSU \\(i\\) in stratum \\(h\\). Similarly, the size estimate in a subpopulation, defined by a dichotomous variable \\(I(y_{hik} = d)\\), which takes the value one if unit \\(k\\) from PSU \\(i\\) in stratum \\(h\\) belongs to category \\(d\\) in the discrete variable \\(y\\), is given by the following expression: \\[ {\\widehat{N}}_d = \\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ I(y_{hik} = d) \\] "],["totals-means-proportions-and-ratios.html", "3.2 Totals, means, proportions, and ratios", " 3.2 Totals, means, proportions, and ratios For single numeric survey variables, the simplest estimates are for totals and means. Ratios are often used to obtain summaries that relate two numeric variables. Estimates for such parameters can be obtained either for the entire population or disaggregated by domains of interest, depending on the research needs. As mentioned by Steven G. Heeringa, West, and Berglund (2017a), the estimation of population totals or averages for a variable of interest, along with the estimation of corresponding variances, has played a crucial role in the development of probability sampling theory. Estimators of population means, proportions and ratios are all dependent on estimating component population totals, as we show in the sequence. 3.2.1 Estimating totals Once the sampling design is defined, which was done in the previous section, the estimation process for the parameters of interest is carried out. For the estimation of totals with complex sampling designs that include stratification \\(\\left(h=1,2,...,H\\right)\\) and subsampling in PSUs (assumed to be within stratum \\(h\\)) indexed by \\(i=1,2,...,m_h\\), the estimator for the population total can be written as: \\[ \\widehat{Y} = \\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ y_{hik} \\] Under full response, the Ultimate Cluster variance estimator for \\(\\widehat{Y}\\) was provided in section 9.1. Calculating the total estimate and its estimated variance is complex, but now these calculations can be easily performed using the svytotal function from the survey package in R. The confidence interval of level \\(1 - \\alpha\\) is given by the following expression: \\[ \\widehat{Y} \\pm z_{1 - \\alpha/2} \\times \\sqrt{\\widehat{V}_{UC} \\left( \\widehat{Y}\\right)} \\] with \\(z_{1 - \\alpha/2}\\) denoting the quantile of the Gaussian distribution leaving an area of \\({\\alpha/2}\\) to its right. 3.2.2 Estimating averages The estimation of the population means or averages is a very important task in household surveys. According to H. A. Gutiérrez (2016), an estimator of the population mean can be written as the ratio of two estimated finite population totals, as follows: \\[ \\widehat{\\overline{Y}} = \\frac{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik}y_{hik}} {\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik}} = \\frac{\\widehat{Y}}{\\widehat{N}}. \\] Since \\(\\widehat{\\overline{Y}}\\) is a nonlinear statistic, there is no closed-form formula for exact the variance of this estimator. For this reason, either resampling methods or Taylor series approximations must be used. The latter may be achieved remembering that for the survey mean the sampling estimating equation requires defining \\(\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} (y_{hik} - \\theta) = 0\\), therefore we can apply the variance estimator given in section 9.1 with \\(z_{hik} = y_{hik} - \\widehat{\\overline{Y}}\\). 3.2.3 Estimating proportions When \\(y\\) is a binary variable, the weighted mean estimates the population proportion. As mentioned by Steven G. Heeringa, West, and Berglund (2017b), by recoding the original response categories into simple indicator variables \\(y\\) with possible values of 1 and 0 (e.g., 1=Yes, 0=No), the estimator for a proportion is defined as follows: \\[ \\widehat{p}_d = \\frac{\\widehat{N}_d}{\\widehat{N}} = \\frac{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik}\\ I(y_{hik} = d)} {\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik}} \\] We can apply Taylor linearization to obtain the approximate variance of the above estimator by defining the estimating function as \\(z_{hik} = I(y_{hik} = d) - \\widehat{p}_d\\). Many statistical packages provide proportion estimates and standard errors on a percentage scale. As is well known in the specialized literature, when the estimated proportion of interest is close to zero or to one, the limits of the traditional symmetric normal confidence intervals may fall outside the permissible range for proportions. This would have no interpretation due to the nature of the parameter. To address this issue, alternative confidence interval estimates, as proposed by Rust, Hsu, and Westat (2007) and Dean and Pagano (2015) are available. One alternative based on using the logit transformation of the estimated proportion is: \\[ CI \\left( \\widehat{p}_d \\ ; \\ 1 - \\alpha \\right) = \\frac {exp \\left[ ln \\left( \\frac{\\widehat{p}_d} {1 - \\widehat{p}_d} \\right) \\pm \\frac{t_{1-\\alpha/2 , \\, df} \\times se \\left( \\widehat{p}_d \\right)} {\\widehat{p}_d \\left( 1 - \\widehat{p}_d \\right) } \\right]} {1 + exp \\left[ ln \\left( \\frac{\\widehat{p}_d} {1 - \\widehat{p}_d} \\right) \\pm \\frac{t_{1-\\alpha/2, \\, df} \\times se \\left( \\widehat{p}_d \\right)} {\\widehat{p}_d \\left( 1 - \\widehat{p}_d \\right)}\\right]} \\] 3.2.4 Estimating ratios In many household survey analyses, it is not sufficient to examine individual variables in isolation. For example, ODS indicator N.17.6.1 is defined as the ratio of the number of broadband subscriptions per 100 inhabitants in a country or region. Ratio estimators are obtained simply by the ratio of the corresponding estimators of totals (or means) in the numerator and denominator. Another example is estimating the ratio of expenditures to income or the ratio of a particular type of expenditure (say food) over total expenditures in a household budget survey. Since the ratio is the quotient of two totals, both the numerator and the denominator are unknown quantities and thus need to be estimated. The point estimator for a ratio in complex surveys is the quotient of the estimators for the totals, as defined by: \\[ \\widehat{R} = \\frac{\\widehat{Y}}{\\widehat{X}} = \\frac{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ y_{hik}}{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ x_{hik}} \\] For variance estimation, all you need to do is specify the estimating function as \\(z_{hik} = y_{hik} - \\widehat{R} \\ x_{hik}\\), when \\(y\\) and \\(x\\) are the numerator and denominator variables, respectively. References Dean, Natalie, and Marcello Pagano. 2015. “Evaluating Confidence Interval Methods for Binomial Proportions in Clustered Surveys.” Journal of Survey Statistics and Methodology 3 (4): 484–503. https://doi.org/10.1093/jssam/smv024. Gutiérrez, H. A. 2016. Estrategias de Muestreo: Diseño de Encuestas y Estimación de Parámetros. Segunda edición. Ediciones de la U. Heeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2017a. Applied Survey Data Analysis. Chapman and Hall CRC Statistics in the Social and Behavioral Sciences Series. CRC Press. Heeringa, Steven G, Brady T West, and Patricia A Berglund. 2017b. Applied Survey Data Analysis, Second Edition. Statistics in the Social and Behavioral Sciences. 2nd edition. Chapman; Hall - CRC. Rust, Keith F., Valerie Hsu, and Westat. 2007. “Confidence Intervals for Statistics for Categorical Variables from Complex Samples.” In. https://api.semanticscholar.org/CorpusID:195852485. "],["variances-and-standard-deviations.html", "3.3 Variances and standard deviations", " 3.3 Variances and standard deviations Sometimes the interest lies in estimating the variance or standard deviation of a numeric survey variable \\(y\\). This can be accomplished using the following estimators: \\[ \\widehat{S_y^2} = \\frac{1}{\\widehat{N}-1} {\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ \\left( y_{hik} - \\widehat{\\overline{Y}} \\right)^2} \\] and \\(\\widehat{S_y} = \\sqrt{\\widehat{S_y^2}}\\) for the standard deviation. "],["correlations.html", "3.4 Correlations", " 3.4 Correlations Pearson correlation coefficients are useful for assessing the relationship between two numeric survey variables, say \\(x\\) and \\(y\\). These can be estimated using \\[ \\widehat{\\rho}_{xy} = \\frac {\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ \\left( y_{hik} - \\widehat{\\overline{Y}} \\right) \\left( x_{hik} - \\widehat{\\overline{X}} \\right)} {\\sqrt{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ \\left( y_{hik} - \\widehat{\\overline{Y}} \\right)^2} \\sqrt{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} \\ \\left( x_{hik} - \\widehat{\\overline{X}} \\right)^2}} \\] "],["percentiles-and-inequality-measures.html", "3.5 Percentiles and inequality measures", " 3.5 Percentiles and inequality measures Non-central location measures are helpful to determine location and spread of the data distribution beyond central values. Key non-central location measures include the quartiles and other quantiles or percentiles. As an example, the estimation of income percentiles in a given country may help define the onset of public policy. For example, a tax could be imposed on individuals in the top 10% of the income distribution, or transport subsidies could be provided to those in the bottom 15% of the income distribution. Quantile estimation is based on results related to weighted total estimators, by first estimating the population cumulative distribution function (CDF). The CDF for a variable \\(y\\) in a finite population of size \\(N\\) is defined as follows: \\[ F(t) = \\frac{1}{N}\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} I(y_{hik} \\leq t) \\] Where \\(I(y_k \\leq x)\\) is an indicator variable that takes the value 1 if \\(y_{hik}\\) is less than or equal to a specific value \\(t\\), and 0 otherwise. An estimator of the CDF in a complex sampling design is given by: \\[ \\widehat{F}(t) = \\frac{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik} I(y_{hik}\\leq t)}{\\sum_{h=1}^{H}\\sum_{i \\in s_{1h}} \\sum_{ k \\in s_{hi}} w_{hik}} \\] Once the CDF is estimated using the survey design weights, the \\(q\\)-th quantile of a variable \\(y\\) is the smallest value of \\(y\\) such that the CDF is greater than or equal to \\(q\\). As is well known, the median is the value where the CDF is greater than or equal to \\(1/2\\). Thus, the estimated median is the smallest value where the estimated CDF is greater than or equal to \\(1/2\\). Following Steven G. Heeringa, West, and Berglund (2017b), to estimate quantiles, one first considers the order statistics denoted as \\(y_{(1)}, \\ldots, y_{(n)}\\) and finds the value of \\(j\\) \\((j=1, \\ldots, n)\\) such that: \\[ \\widehat{F}(y_{(j)}) \\leq q\\leq\\widehat{F}(y_{(j+1)}) \\] Hence, the estimator of the \\(q\\)-th quantile \\(y_{(q)}\\) is given by: \\[ \\widehat{y}_{(q)} = y_{(j)} + \\frac{q - \\widehat{F}(y_{(j)})}{\\widehat{F}(y_{(j+1)}) - \\widehat{F}(y_{(j)})} (y_{(j+1)} - y_{(j)}) \\] For the variance estimation and confidence intervals of quantiles, Kovar, Rao, and Wu (1988) present results from a simulation study where they recommend using the Balanced Repeated Replication (BRR) technique. 3.5.1 Estimating the Gini coefficient Economic inequality is a common issue worldwide, with particular focus from international institutions. Measuring economic inequality among households is of great interest, and the Gini coefficient (\\(G\\)) is the most commonly used indicator for this purpose. The Gini coefficient ranges from 0 to 1, where \\(G = 0\\) indicates perfect equality in wealth distribution, and higher values reflect increasing inequality. Following the estimation equation proposed by David A. Binder and Kovacevic (1995), the estimator for the Gini coefficient is given by: \\[ \\widehat{G} = \\frac {2 \\times \\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik}^{*} \\widehat{F}_{hik} \\ y_{hik} - 1} {\\widehat{\\overline{Y}}} \\] where \\(w_{hik}^{*}\\) is a normalized sampling weight, defined as \\[ w_{hik}^{*} = \\frac{w_{hik}} {\\sum_{h=1}^{H} \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik}} \\] and \\(\\widehat{F}_{hik}\\) represents the estimated CDF for individual \\(k\\) in cluster \\(i\\) of stratum \\(h\\). Osier (2009) and Langel and Tillé (2013) provide important computational details for estimating the variance of this complex estimator. References Binder, David A., and Milojica S. Kovacevic. 1995. “Estimating Some Measures of Income Inequality from Survey Data: An Application of the Estimating Equations Approach.” Survey Methodology 21 (2): 137–45. Heeringa, Steven G, Brady T West, and Patricia A Berglund. 2017b. Applied Survey Data Analysis, Second Edition. Statistics in the Social and Behavioral Sciences. 2nd edition. Chapman; Hall - CRC. Kovar, J. G., J. N. K. Rao, and C. F. J. Wu. 1988. “Bootstrap and Other Methods to Measure Errors in Survey Estimates.” Canadian Journal of Statistics 16 (Suppl.): 25–45. Langel, Matti, and Yves Tillé. 2013. “Variance Estimation of the Gini Index: Revisiting a Result Several Times Published: Variance Estimation of the Gini Index.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 176 (2): 521–40. https://doi.org/10.1111/j.1467-985X.2012.01048.x. Osier, Guillaume. 2009. “Variance Estimation for Complex Indicators of Poverty and Inequality.” Journal of the European Survey Research Association 3 (3): 167–95. http://ojs.ub.uni-konstanz.de/srm/article/view/369. "],["nso-practical-example.html", "3.6 NSO – Practical example", " 3.6 NSO – Practical example In this subsection a NSO will share how they do disseminate its results on basic descriptive statistics, how they publish the resulting tables and how do they deal with the suppression of estimates that do not reach expected quality. "],["associations-between-variables.html", "4 Associations between variables", " 4 Associations between variables Household sample surveys often collect data on categorical variables, and assessing whether pairs of these variables are associated becomes of interest. This section will introduce the reader on the main methods currently used to describe and infer associations for pairs of categorical variables. We start by defining some notation. Let \\(x\\) and \\(y\\) denote two categorical variables, having \\(R\\) and \\(C\\) classes respectively. In order to formulate hypothesis tests for the independence between \\(x\\) and \\(y\\), we need to consider a superpopulation model. We assume that the pairs \\((x_{hik} , y_{hik})\\) correspond to observations from identically distributed random vectors \\((X ; Y)\\), that have joint distribution specified by \\[ P_{rc} = Pr \\left( X = r \\ ; \\ Y = c \\right) \\quad \\text{for } r=1,...,R \\text{ and } c=1,...,C \\] with \\(\\sum_r \\sum_c P_{rc} = 1\\). If a census could be carried out to collect data on \\(x\\) and \\(y\\) from every unit in the population, we could calculate the population counts of units having classes \\((r,c)\\) for \\((x,y)\\) given by: \\[ N_{rc} = \\sum_{h=1}^H \\sum_{i \\in U_{1h}} \\sum_{k \\in U_{hi}} I \\left( x_{hik} = r \\ ; \\ y_{hik} = c \\right) \\] and the corresponding population proportions as \\(p_{rc} = N_{rc} / N_{++}\\), where \\(N_{++} = \\sum_r \\sum_c N_{rc}\\) denotes the total number of units in the population. Under the superpopulation model, the population proportions \\(p_{rc}\\) could be used to estimate (or approximate) the unknown probabilities \\(P_{rc}\\). Since in most instances we will have samples, not censuses, the population proportions \\(p_{rc}\\) must be estimated using weighted estimators provided in the previous sections. "],["cross-tabulations-and-contingency-tables.html", "4.1 Cross-tabulations and contingency tables", " 4.1 Cross-tabulations and contingency tables Cross-tabulations organize population frequency distribution estimates for two or more categorical variables to help explore relationships between them. Tests of independence can be used to assess whether the cross-classified variables are related or independent. This type of analysis is important in many research and decision-making settings. In the specialized literature, cross-tabulations are also referred to as contingency tables. Here a table is a two-dimensional array with rows indexed by \\(r=1,\\ldots,R\\) and columns indexed by \\(c=1,\\ldots,C\\). Such tables are widely used in household survey analysis as they summarize the relationship between categorical variables in terms of frequency counts. A contingency table aims to succinctly represent the association between different categorical variables. First we consider the case of a two-way contingency table. For most household sample surveys, a typical tabular output comprises the weighted frequencies that estimate the population frequencies, as follows: \\(y\\) \\(x\\) 1 \\(\\ldots\\) \\(C\\) row marg. 1 \\(\\widehat{N}_{11}\\) \\(\\ldots\\) \\(\\widehat{N}_{1C}\\) \\(\\widehat{N}_{1+}\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\widehat{N}_{rc}\\) \\(\\ldots\\) \\(\\ldots\\) \\(R\\) \\(\\widehat{N}_{R1}\\) \\(\\ldots\\) \\(\\widehat{N}_{RC}\\) \\(\\widehat{N}_{R+}\\) col. marg. \\(\\widehat{N}_{+1}\\) \\(\\ldots\\) \\(\\widehat{N}_{+C}\\) \\(\\widehat{N}\\) where the estimated frequency in cell \\((r,c)\\) is obtained as \\[ \\widehat{N}_{rc} = \\sum_{h=1}^H \\sum_{i \\in s_{1h}} \\sum_{k \\in s_{hi}} w_{hik} \\ I \\left( x_{hik} = r \\ ; \\ y_{hik} = c \\right) \\] and \\(\\widehat{N}_{r+} = \\sum_c \\widehat{N}_{rc}\\), \\(\\widehat{N}_{+c} = \\sum_r \\widehat{N}_{rc}\\) and \\(\\widehat{N}_{++} = \\sum_r \\sum_c \\widehat{N}_{rc}\\). The estimated proportions from these weighted sample frequencies are obtained as follows: \\[ \\widehat{p}_{rc} = \\frac{\\widehat{N}_{rc}}{\\widehat{N}_{++}} \\] \\(\\widehat{p}_{r+} = \\sum_c \\widehat{N}_{r+} / \\widehat{N}_{++}\\), and \\(\\widehat{p}_{+c} = \\sum_r \\widehat{N}_{rc} / \\widehat{N}_{++}\\). Two-way tables can also display the estimates of population relative frequencies, as shown below: \\(y\\) \\(x\\) 1 \\(\\ldots\\) \\(C\\) row marg. 1 \\(\\widehat{p}_{11}\\) \\(\\ldots\\) \\(\\widehat{p}_{1C}\\) \\(\\widehat{p}_{1+}\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\widehat{p}_{rc}\\) \\(\\ldots\\) \\(\\ldots\\) \\(R\\) \\(\\widehat{p}_{R1}\\) \\(\\ldots\\) \\(\\widehat{p}_{RC}\\) \\(\\widehat{p}_{R+}\\) col. marg. \\(\\widehat{p}_{+1}\\) \\(\\ldots\\) \\(\\widehat{p}_{+C}\\) \\(1\\) "],["testing-for-independence.html", "4.2 Testing for independence", " 4.2 Testing for independence Using the estimated contingency tables, it is possible to perform independence tests to verify whether \\(x\\) and \\(y\\) are associated. Following Steven G. Heeringa, West, and Berglund (2017b), the null hypothesis that \\(x\\) and \\(y\\) are independent is defined as: \\[ H_0) \\ \\ P_{rc}^0 = {P_{r+} \\times P_{+c}} \\ \\ \\forall \\ r=1, \\ldots, R \\text{ and } c=1, \\ldots, C. \\] Hence, to test the independence hypothesis we compare the estimated proportions \\(\\widehat{p}_{rc}\\) with the estimated expected population proportions under the null \\(P_{rc}^0\\). If there is a large difference between them, then the independence hypothesis would not be supported by the data. Therefore, the following Pearson Rao-Scott adjusted test statistic \\(X_{RS}^2\\) (Rao and Scott 1984) is defined: \\[ X_{RS}^2 =\\ \\frac {n_{++}} {GDEFF} \\sum_r \\sum_c \\frac{\\left(\\widehat{p}_{rc} - \\widehat P_{rc}^0 \\right)^2}{\\widehat P_{rc}^0} \\] where \\(\\widehat P_{rc}^0 = \\widehat p_{r+} \\times \\widehat p_{+c}\\) estimates the cell frequencies under the null hypothesis and \\(GDEFF\\) is an estimate of the generalized design effect given by \\[ GDEFF = \\frac{\\sum_{r}\\sum_{c}\\left(1 - \\widehat p_{rc}\\right) d^{2}\\left(\\widehat p_{rc}\\right) - \\sum_{r} \\left(1 - \\widehat p_{r+} \\right) d^{2}\\left(\\widehat p_{r+}\\right) - \\sum_{c} \\left( 1 - \\widehat p_{+c} \\right) d^{2} \\left( \\widehat p_{+c} \\right)} {\\left(R-1\\right)\\left(C-1\\right)} \\] where \\(d^2 \\left({\\widehat \\theta}\\right)\\) denotes the estimated design effect for the estimator \\({\\widehat \\theta}\\). Under the null hypothesis of independence, the large sample distribution of \\(X_{RS}^2\\) is \\(\\chi^2_{[(R-1) (C-1)]}\\). As mentioned by Steven G. Heeringa, West, and Berglund (2017b), it was Fay (1979), along with Fellegi (1980), who began proposing corrections to Pearson’s chi-square statistic based on a generalized design effect. Rao and Scott (1984) later expanded the theory of generalized design effect corrections for these statistical tests, as did Thomas and Rao (1987). The Rao-Scott adjustment requires the calculation of generalized design effects, which are analytically more complex than Fellegi’s approach. Nevertheless, Rao-Scott adjusted statistics are now the standard for analyzing categorical survey data in software systems such as R, Stata and SAS. The Rao-Scott adjusted Likelihood Ratio statistic is defined as: \\[ G_{RS}^{2} = 2 \\times \\frac {n_{++}} {GDEFF} \\times \\sum_{r} \\sum_{c} \\widehat p_{rc} \\times \\ln \\left( \\frac{\\widehat p_{rc}}{p_{rc}^0} \\right) \\] Under the null hypothesis of independence, the large sample distribution of this test statistic is also \\(\\chi^2_{[(R-1) (C-1)]}\\). When the number of degrees of freedom for the sample is not very large, two adjusted versions of the above test statistics might be preferable, since they enable taking this into account. The F-adjusted test statistic for independence based on Pearson’s \\(X_{RS}^2\\) is calculated as follows: \\[ F_{Pearson} = \\frac{X^{2}_{R-S}}{(R-1)(C-1)} \\sim F_{\\left(R-1\\right)\\left(C-1\\right) \\ , \\ df} \\] where \\(df = \\sum_{h} n_h - H\\) denotes the degrees of freedom in the design. The F-adjusted teststatistic for independence based on the Rao-Scott adjusted Likelihood Ratio statistic \\(G_{RS}^{2}\\) is calculated as: \\[ F_{LR} = \\frac{G^{2}_{R-S}}{C-1} \\sim F_{\\left(C-1\\right) \\ , \\ df} \\] References Fay, R. E. 1979. “On Adjusting the Pearson Chi-Square Statistic for Clustered Sampling.” ASA Proceedings of the Social Statistics Section, 402–8. Fellegi, Ivan P. 1980. “Approximate Joint Estimation of the Parameters of Multinomial Distributions in the Analysis of Data from Complex Surveys.” Journal of the American Statistical Association 75 (370): 261–68. Heeringa, Steven G, Brady T West, and Patricia A Berglund. 2017b. Applied Survey Data Analysis, Second Edition. Statistics in the Social and Behavioral Sciences. 2nd edition. Chapman; Hall - CRC. Rao, J. N. K., and A. J. Scott. 1984. “On Chi-Squared Tests for Multiway Contingency Tables with Cell Proportions Estimated from Survey Data.” The Annals of Statistics 12: 46–60. Thomas, D. R., and J. N. K. Rao. 1987. “Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics Under Cluster Sampling.” Journal of the American Statistical Association 82: 630–36. "],["tests-for-group-comparisons.html", "4.3 Tests for group comparisons", " 4.3 Tests for group comparisons To determine whether the means of two groups are significantly different we will introduce t-test and contrasts adjusted for the sampling design. 4.3.1 Hypothesis Test for the Difference of Means A hypothesis test is a statistical procedure used to evaluate evidence in favor of or against a statement or assumption about a population. In this process, a null hypothesis (\\(H_0\\)) is proposed, representing the initial statement that needs to be tested, and an alternative hypothesis (\\(H_1\\)), which is the statement opposing the null hypothesis. These statements may be based on some belief or past experience and will be tested using the evidence gathered from the survey data. If it is suspected that the parameter \\(\\theta\\) is equal to a particular value \\(\\theta_{0}\\), the possible combinations of hypotheses that can be tested are: \\[\\begin{eqnarray*} \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta\\neq\\theta_{0} \\end{cases}\\,\\,\\,\\,\\,\\,\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&gt;\\theta_{0} \\end{cases}\\,\\,\\,\\,\\,\\,\\,\\,\\, \\begin{cases} H_{0}: &amp; \\theta=\\theta_{0}\\\\ H_{1}: &amp; \\theta&lt;\\theta_{0} \\end{cases} \\end{eqnarray*}\\] One of the two hypotheses will be considered true only if the statistical evidence, which is obtained from the sample, supports it. The process of selecting one of the two hypotheses is called a Hypothesis Test. In general, some important parameters can be expressed as a linear combination of measures of interest. The most common cases are differences in means, weighted sums of means used to construct economic indices, etc. Thus, consider a function that is a linear combination of \\(J\\) descriptive statistics, as shown below: \\[ f \\left( \\theta_{1}, \\ldots,\\theta_{J}\\right) = \\sum_{j=1}^{J}a_{j}\\theta_{j} \\] where the \\(a_{j}\\) are known constants. An estimator of this function is given by: \\[ \\widehat{f} \\left( \\widehat{\\theta}_{1}, \\ldots, \\widehat{\\theta}_{J} \\right) = \\sum_{j=1}^{J} a_{j} \\widehat{\\theta}_{j} \\] And its variance is calculated as follows: \\[ Var \\left( \\sum_{j=1}^{J} a_{j} \\widehat{\\theta}_{j} \\right) = \\sum_{j=1}^{J} a_{j}^{2} \\ Var\\left( \\widehat{\\theta}_{j} \\right) + 2 \\times \\sum_{j=1}^{J-1} \\sum_{k&gt;j}^{J} a_{j} a_{k} \\, Cov \\left( \\widehat{\\theta}_{j} , \\widehat{\\theta}_{k} \\right) \\] As seen in the variance expression for the estimator, it requires the variances of the individual estimators, as well as the covariances of pairs of estimators. Of particular interest is analyzing the difference in population means. In order to formulate the hypothesis tests for this case, we need to consider a superpopulation model. We assume that \\(y_{hik}\\) correspond to observations from identically distributed random variables \\(Y\\) having means \\(\\mu_{y,j}\\) if unit \\(k\\) belongs to domain \\(j\\), with \\(j = 1, 2\\). Then we can define the difference in population means between domains 1 and 2 as \\(\\mu_{y,1} - \\mu_{y,2}\\). As an example, consider that \\(\\mu_{y,1}\\) is the average household income for households with male heads of household, and \\(\\mu_{y,2}\\) is the average household income for households with female heads. This difference in means can be unbiasedly estimated by: \\[ \\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2} \\] where \\(\\widehat{\\overline{Y}}_{j}\\) is the sample estimator of \\(\\mu_{y,j}\\) (\\(j = 1, 2\\)). Considering the parameter of interest in this section, the hypotheses to be tested are as follows: \\[ \\begin{cases} H_0: \\mu_{y,1} - \\mu_{y,2} = 0 \\\\ H_1: \\mu_{y,1} - \\mu_{y,2} \\neq 0 \\end{cases} \\] \\[ \\begin{cases} H_0: \\mu_{y,1} - \\mu_{y,2} = 0 \\\\ H_1: \\mu_{y,1} - \\mu_{y,2} &gt; 0 \\end{cases} \\] \\[ \\begin{cases} H_0: \\mu_{y,1} - \\mu_{y,2} = 0 \\\\ H_1: \\mu_{y,1} - \\mu_{y,2} &lt; 0 \\end{cases} \\] To test one of these hypothesis, the following test statistic is used, which follows a t-student distribution with \\(df\\) degrees of freedom, calculated as the difference between the number of PSUs (Primary Sampling Units) and the number of strata. \\[ t = \\frac{\\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2}} {se\\left(\\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2}\\right)} \\sim t_{[df]} \\] Where: \\[ \\widehat{se} \\left( \\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2} \\right) = \\sqrt{\\widehat{Var}\\left(\\widehat{\\overline{Y}}_{1}\\right) + \\widehat{Var}\\left(\\widehat{\\overline{Y}}_{2}\\right) - 2 \\ \\widehat{Cov}\\left(\\widehat{\\overline{Y}}_{1} \\ ; \\widehat{\\overline{Y}}_{2} \\right)} \\] If a confidence interval for the difference in means is desired, it would be constructed as follows: \\[ \\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2} \\ \\pm \\ t_{[df]} \\ \\widehat{se}\\left( \\widehat{\\overline{Y}}_{1} - \\widehat{\\overline{Y}}_{2} \\right) \\] "],["nso-practical-example-1.html", "4.4 NSO – Practical example", " 4.4 NSO – Practical example In this part an NSO will share its experiences on dealing with statistical comparisons among groups and how do they present the results in tables. "],["regression-modelling-survey-data.html", "5 Regression: modelling survey data", " 5 Regression: modelling survey data Modelling survey data is a common task among researcher; some of them include the features of the sampling design in computing standard error of the estimated regression parameters. In this section we will deal with the problem of weighting in regression models and present a parsimonious solution. "],["to-weight-or-not-to-weight.html", "5.1 To weight or not to weight?", " 5.1 To weight or not to weight? We present the pros and cons of including the complex design features in the estimation of regression parameters and their associated standard errors. We present some adjustment to the sampling weights to fit these kind of models (senate sampling weights, normalized sampling weights, Pfeffermann model weights). "],["some-inferential-approaches-to-modelling-data.html", "5.2 Some inferential approaches to modelling data", " 5.2 Some inferential approaches to modelling data When modelling survey data, one should deal with two sources of variability: the one devoted to the complex sampling design and the one that comes from the very model. Combining these sources into a valid set up requires of some advanced methods. We will mention some of them: pseudo likelihood, combined inference. "],["linear-models.html", "5.3 Linear models", " 5.3 Linear models 5.3.1 Basic Definitions As noted by Steven G. Heeringa, West, and Berglund (2017a), the first authors to empirically discuss the impact of complex sampling designs on regression model inferences were Kish and Frankel (1974). Later, Fuller (1975) developed a variance estimator for regression model parameters based on Taylor linearization with unequal weighting of observations under stratified and two-stage sampling designs. As is well known, the use of regression model theory requires certain statistical assumptions to be met, which can sometimes be challenging to verify in practice. In this regard, B. V. Shah, Holt, and Folsom (1977) discuss some aspects related to the violation of these assumptions and provide appropriate methods for making inferences about the estimated parameters of linear regression models using survey data. Similarly, David A. Binder (1983a) obtained the sampling distributions of estimators for regression parameters in finite populations and related variance estimators in the context of complex samples. Skinner, Holt, and Smith (1989) studied the properties of variance estimators for regression coefficients under complex sample designs. Later, Fuller (2002) provided a summary of estimation methods for regression models containing information related to complex samples. Finally, Pfeffermann (2011) discussed various approaches to fitting linear regression models to complex survey data, presenting empirical support for the use of the “q-weighted” method, which is recommended in this document. A simple linear regression model is defined as \\(y=\\beta_{0}+\\beta_{1}x+\\varepsilon\\), where \\(y\\) represents the dependent variable, \\(x\\) is the independent variable, and \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the model parameters. The variable \\(\\varepsilon\\) is known as the random error of the model and is defined as \\(\\varepsilon=y-\\hat{y}=y-\\beta_{0}+\\beta_{1}x\\). Generalizing the previous model, multiple linear regression models are defined by allowing the dependent variable to interact with more than two variables, as presented below: \\[ y = \\boldsymbol{x}\\boldsymbol{\\beta}+\\varepsilon = \\sum_{j=0}^{p}\\beta_{j}x_{j}+\\varepsilon = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p}+\\varepsilon \\] Another way to write the multiple regression model is: \\[ y_{i} = x_{i}\\boldsymbol{\\beta}+\\varepsilon_{i} \\] Where, \\(x_{i}=\\left[1\\,x_{1i}\\,\\ldots\\,x_{pi}\\right]\\) and \\(\\boldsymbol{\\beta}^{T}=\\left[\\beta_{0}\\,\\,\\beta_{1}\\,\\,\\ldots\\,\\,\\beta_{p}\\right]\\). The subscript \\(i\\) refers to the sample element or respondent in the dataset. Steven G. Heeringa, West, and Berglund (2017a) present some considerations for regression models, which are described below: \\(E\\left(\\varepsilon_{i}\\mid x_{i}\\right)=0\\), meaning that the expected value of the residuals conditioned on the covariates is zero. \\(Var\\left(\\varepsilon_{i}\\mid x_{i}\\right)=\\sigma_{y,x}^{2}\\) (homogeneity of variance), meaning that the variance of the residuals conditioned on the covariates is constant. \\(\\varepsilon_{i}\\mid x_{i}\\sim N\\left(0,\\,\\sigma_{y,x}^{2}\\right)\\) (normality of errors), meaning that the residuals conditioned on the covariates follow a normal distribution. This property also extends to the response variable \\(y_{i}\\). \\(cov\\left(\\varepsilon_{i},\\,\\varepsilon_{j}\\mid x_{i},x_{j}\\right)\\) (independence of residuals), meaning that the residuals in different observed units are not correlated with the values given by their predictor variables. Once the linear regression model and its assumptions are defined, it can be deduced that the best unbiased linear estimator is defined as the expected value of the dependent variable conditioned on the independent variables \\(x\\), as: \\[ E\\left(y\\mid x\\right)=\\hat{\\beta}_{0}+\\hat{\\beta_{1}}x_{1}+\\hat{\\beta}_{2}x_{2}+\\cdots+\\hat{\\beta}_{p}x_{p} \\] \\[ \\hat{y} = E\\left(y\\mid x\\right) = E\\left(\\boldsymbol{x}\\boldsymbol{\\beta}\\right)+E\\left(\\varepsilon\\right) = \\boldsymbol{x}\\boldsymbol{\\beta}+0 = \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{p}x_{p} \\] Additionally, \\[ var\\left(y_{i}\\mid x_{i}\\right) = \\sigma_{y,x}^{2} \\] It is also established that: \\[ cov\\left(y_{i},y_{j}\\mid x_{i},x_{j}\\right) = 0 \\] Thus, the response variable has the following distribution: \\[ y_{i} \\sim N\\left(x_{i}\\boldsymbol{\\beta},\\sigma_{y,x}^{2}\\right) \\] 5.3.2 Estimation of Parameters in a Regression Model with Complex Samples Once the assumptions of the model and the distributional characteristics of the errors are established, the next step is the process of parameter estimation. As an illustrative and introductory example, if instead of observing a sample of size \\(n\\) from the \\(N\\) elements of the population, a complete census had been conducted, the finite population regression parameter \\(\\beta_{1}\\) could be calculated as follows: \\[ \\beta_{1} = \\frac{ \\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{N}\\left(X_{i}-\\bar{X}\\right)^{2}} \\] Now, when estimating the parameters of a linear regression model considering that the observed information comes from surveys with complex samples, the standard approach to estimating regression coefficients and their standard errors is altered. The main reason for this change is that data collected through a complex survey generally does not have an identical distribution, and the assumption of independence cannot be maintained since the sample design is constructed with dependencies (as most complex designs include stratification, clustering, unequal selection probabilities, etc.). In this context, when fitting regression models with such datasets, using conventional estimators derived from traditional methods (such as maximum likelihood, for example) will induce bias because these methods assume the data are independently and identically distributed and come from a specific probability distribution (binomial, Poisson, exponential, normal, etc.). Instead, according to Wolter (2007), robust non-parametric methods based on Taylor linearization or variance estimation methods using replication (Jackknife, bootstrapping, etc.) are used to eliminate bias by including the sampling design in the analyses. For illustrative purposes, the estimation of the parameter \\(\\beta_{1}\\) and its variance for a simple linear regression will be shown. The extension to multiple regression parameter estimation is algebraically complex and beyond the scope of this book. Below is the estimation of the slope and its variance in a simple linear regression model: \\[ \\hat{\\beta_{1}} = \\frac{\\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-\\hat{\\bar{y}}_{\\omega}\\right)\\left(x_{h\\alpha i}-\\hat{\\bar{x}}_{\\omega}\\right)}{\\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(x_{h\\alpha i}-\\hat{\\bar{x}}_{\\omega}\\right)^{2}} \\] As it can be seen in the above equation, the parameter estimator is a ratio of totals; therefore, its variance is given by: \\[ var\\left(\\hat{\\beta_{1}}\\right) = \\frac{var\\left(\\hat{t}_{xy}\\right)+\\hat{\\beta}_{1}^{2}var\\left(\\hat{t}_{x^{2}}\\right)-2\\hat{\\beta}_{1}cov\\left(\\hat{t}_{xy},\\hat{t}_{x^{2}}\\right)}{\\left(\\hat{t}_{x^{2}}\\right)^{2}} \\] As a generalization, according to Kish and Frankel (1974), the variance estimation of coefficients in a multiple linear regression model requires weighted totals for the squares and cross-products of all combinations of \\(y\\) and \\(x = \\{1, x_{1}, \\ldots, x_{p}\\}\\). Below is the estimation of these variances: \\[ \\text{var}\\left(\\hat{\\beta}\\right) = \\hat{\\Sigma}\\left(\\hat{\\beta}\\right) = \\begin{bmatrix} \\text{var}\\left(\\hat{\\beta}_{0}\\right) &amp; \\text{cov}\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; \\text{cov}\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right) \\\\ \\text{cov}\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{1}\\right) &amp; \\text{var}\\left(\\hat{\\beta}_{1}\\right) &amp; \\cdots &amp; \\text{cov}\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\text{cov}\\left(\\hat{\\beta}_{0},\\hat{\\beta}_{p}\\right) &amp; \\text{cov}\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{p}\\right) &amp; \\cdots &amp; \\text{var}\\left(\\hat{\\beta}_{p}\\right) \\end{bmatrix} \\] 5.3.3 The Pfeffermann Weighting Approach Steven G. Heeringa, West, and Berglund (2017a) addresses the problem of how to correctly weight regression models and whether expansion factors should be used to estimate regression coefficients when working with complex survey data. In this context, it is essential to know that two primary paradigms exist in the specialized literature: The design-based approach, illustrated in this document, seeks to make inferences about the entire finite population, and the use of expansion factors ensures that regression parameter estimates are unbiased. However, using survey weights does not protect against model misspecification; if the researcher fits a poorly specified model using expansion factors, unbiased estimates of the regression parameters in a model that does not correctly describe the relationships in the finite population are being computed. The population-based modeling approach, which argues that the use of expansion factors in estimation should not be necessary if the model is correctly specified. Under this approach, including survey weights only serves to increase the variance of the estimators, inducing larger-than-necessary standard errors. The choice between these two approaches should depend on the sensitivity of inferences to different estimation methods. It is often recommended to use statistical software to fit regression models with and without survey weights to evaluate the sensitivity of the results. If the use of weights produces substantially different estimates and conclusions, it suggests that the model may be misspecified, and weighted estimates should be preferred. However, if the use of weights does not significantly alter the regression parameter estimates and only considerably increases standard errors, it could indicate that the model is well-specified, and the use of weights may not be necessary. An intermediate solution between these two approaches is given by Pfeffermann (2011), who proposed a variant (called the q-weighted approach) with a slightly different specification of the expansion factors, detailed as follows: Fit a regression model to the final survey weights using the predictor variables in the regression model of interest. Obtain the predicted survey weights for each case as a function of the predictor variables in the dataset. Divide the survey expansion factors by the predicted values from the previous step. Use the new weights obtained for fitting the regression models. This method adjusts the survey weights based on the fitted model, balancing between design-based and model-based approaches to reduce variance while accounting for complex survey design. 5.3.4 Model Diagnostics When fitting statistical models to household survey data, it is essential to perform quality checks to ensure the validity of the conclusions drawn. Most academic texts provide a detailed overview of the assumptions and considerations necessary for a properly defined model. Below are some of the key aspects to consider: Model fit: It is important to determine whether the model provides an adequate fit to the data. Distribution of errors: Examine whether the errors are normally distributed. Error variance: Check whether the errors have constant variance. Error independence: Verify that the errors can be assumed to be uncorrelated. Influential data points: Identify if any data points have an unusually large influence on the estimated regression model. Outliers: Detect points that do not follow the general trend of the data, known as outliers. 5.3.4.1 Coefficient of Determination The coefficient of determination, also known as the multiple correlation coefficient (\\(R^{2}\\)), is a common measure of goodness-of-fit in a regression model. This coefficient estimates the proportion of variance in the dependent variable explained by the model and ranges between 0 and 1. A value close to 1 indicates that the model explains a large proportion of the variability, while a value near 0 suggests the opposite. The calculation of this coefficient for a population is done as follows: \\[ R^{2} = 1-\\frac{SSE}{SST} \\] Where: \\(SST= \\sum_{i=1}^N (y_i - \\bar{y})^2\\): This is the total sum of squares, representing the total variability in the dependent variable. \\(SSE= \\sum_{i=1}^N (y_i - x_i \\beta)^2\\): This is the sum of squared errors, representing the variability not explained by the regression model. For surveys with complex sampling designs, the weighted estimator of \\(R^{2}\\) is given by: \\[ \\widehat{R}_{\\omega}^{2} = 1-\\frac{\\widehat{SSE}_{\\omega}}{\\widehat{SST}_{\\omega}} \\] Where \\(\\widehat{SSE}_{\\omega}\\) is the weighted sum of squared errors, defined as: \\[ \\widehat{SSE}_{\\omega} = \\sum_{h}^{H}\\sum_{\\alpha}^{a_{h}}\\sum_{i=1}^{n_{h\\alpha}}\\omega_{h\\alpha i}\\left(y_{h\\alpha i}-x_{h\\alpha i}\\hat{\\beta}\\right)^{2} \\] This estimator adjusts the \\(R^{2}\\) calculation to reflect the characteristics of the sampling design, such as stratification and unequal selection probabilities, ensuring that survey weights are considered when evaluating the goodness-of-fit of the model. 5.3.4.2 Standardized Residuals In model diagnostics, analyzing residuals is crucial. These analyses provide, under the assumption that the fitted model is adequate, an estimate of the errors. Therefore, a careful study of the residuals should help the researcher conclude whether the fitting process has not violated the assumptions or if, on the contrary, one or more assumptions are not met, requiring a review of the fitting procedure. To analyze the residuals, Pearson residuals (Steven G. Heeringa, West, and Berglund 2017a) are defined as follows: \\[ r_{p_{i}} = \\left(y_{i}-\\mu_{i}\\left(\\hat{\\beta}_{\\omega}\\right)\\right)\\sqrt{\\frac{\\omega_{i}}{V\\left(\\hat{\\mu}_{i}\\right)}} \\] Where \\(\\mu_{i}\\) is the expected value of \\(y_{i}\\), and \\(\\omega_{i}\\) is the survey weight for the i-th individual in the complex sample design. Finally, \\(V(\\mu_{i})\\) is the variance function of the outcome. These residuals are used to perform normality and constant variance analyses. If the assumption of constant variance is not met, the estimators remain unbiased and consistent, but they are no longer efficient. That is, they are no longer the best in the sense that they no longer have the smallest variance among all unbiased estimators. One way to analyze the assumption of constant variance in the errors is through graphical analysis. This is done by plotting the model residuals against \\(\\hat{y}\\) or the model residuals against \\(X_{i}\\). If these plots reveal any pattern other than a constant cloud of points, it can be concluded that the error variance is not constant. 5.3.4.3 Influential Observations Another set of techniques used for model analysis involves examining influential observations. An observation is deemed influential if, when removed from the data set, it causes a significant change in the model fit. It is important to note that an influential point may or may not be an outlier. To detect influential observations, it is essential to clarify what type of influence is being sought. For instance, an observation may be influential for parameter estimation but not for error variance estimation. Below are some statistical techniques for detecting influential data points: Cook’s Distance: This diagnostic measures whether the i-th observation is influential in the model estimation by being far from the data’s center of mass. Various authors consider an observation influential when this value exceeds 2 or 3. \\(D_fBeta_{(i)}\\) Statistic: This statistic measures the change in the estimated regression coefficient vector when the observation is removed. The i-th observation is considered influential for \\(B_j\\) if \\(\\mid D_{f}Betas_{\\left(i\\right)j}\\mid \\geq \\frac{z}{\\sqrt{n}}\\) with \\(z = 2\\). Alternatively, \\(t_{0.025,n-p}/\\sqrt{n}\\) can be used, where \\(t_{0.025,n-p}\\) is the 97.5th percentile. \\(D_{f}Fits_{\\left(i\\right)}\\) Statistic: This statistic measures the change in the model fit when a particular observation is removed. In this case, the i-th observation is considered influential in the model fit if \\(\\mid DfFits\\left(i\\right)\\mid \\geq z\\sqrt{\\frac{p}{n}}\\) with \\(z = 2\\). 5.3.4.4 Inference on Model Parameters Once the proper fit of the model has been evaluated using the methodologies discussed above, and the distributional properties of the errors—and consequently the response variable \\(y\\)—have been verified, the next step is to assess whether the estimated parameters are significant. This involves determining whether the covariates used to fit the model add value in explaining and/or predicting the study variable and the phenomenon of interest. Given the distributional properties of the regression coefficient estimators, a natural test statistic for evaluating the significance of these parameters is based on the t-distribution and is described as follows: \\[ t = \\frac{\\hat{\\beta}_{k}-\\beta_{k}}{se\\left(\\hat{\\beta}_{k}\\right)}\\sim t_{n-p} \\] Where \\(p\\) is the number of model parameters and \\(n\\) is the sample size of the survey. The test statistic above evaluates the hypotheses \\(H_{0}:\\beta_{k}=0\\) versus the alternative \\(H_{1}:\\beta_{k}\\neq0\\). Similarly, a confidence interval of \\((1-\\alpha)\\times100\\%\\) for \\(\\beta_{k}\\) can be constructed, as follows: \\[ \\hat{\\beta}_{k}\\pm t_{1-\\frac{\\alpha}{2},\\,df}\\,se\\left(\\hat{\\beta}_{k}\\right) \\] Where the degrees of freedom (\\(df\\)) for the interval in a household survey (complex samples) is given by the number of final stage clusters minus the number of primary stage strata \\(\\left(df=\\sum_{h}a_{h}-H\\right)\\). 5.3.4.5 Estimation and Prediction According to Neter, Wasserman, and Kutner (1996), linear regression models are essentially used for two purposes. One is to explain the variable of interest in terms of covariates that may be found in surveys, administrative records, censuses, etc. Additionally, they are also used to predict values of the variable under study, either within the range of values collected in the sample or outside of it. The first purpose has been addressed throughout this chapter, and the second is achieved as follows: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\boldsymbol{x}_{obs,i}\\hat{\\boldsymbol{\\beta}} \\] Explicitly, in the model exemplified in this chapter, the expression for predictions would be: \\[ \\hat{E}(y_{i}\\mid\\boldsymbol{x}_{obs,i})=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1i} \\] The variance of the estimation is calculated as follows: \\[ var\\left(\\hat{E}\\left(y_{i}\\mid x_{obs,i}\\right)\\right) = x&#39;_{obs,i}cov\\left(\\hat{\\beta}\\right)x{}_{obs,i} \\] References Binder, David A. 1983a. “On the Variances of Asymptotically Normal Estimators from Complex Surveys.” International Statistical Review 51: 279–92. Fuller, Wayne A. 1975. “Regression Analysis for Sample Survey.” Sankyha, Series C 37: 117–32. ———. 2002. “Regression Estimation for Survey Samples (with Discussion).” Survey Methodology 28 (1): 5–23. Heeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2017a. Applied Survey Data Analysis. Chapman and Hall CRC Statistics in the Social and Behavioral Sciences Series. CRC Press. Kish, Leslie, and Martin R Frankel. 1974. “Inference from Complex Samples.” Journal of the Royal Statistical Society, Series B 36: 1–37. Neter, John, William Wasserman, and Michael H. Kutner. 1996. Applied Linear Statistical Models. McGraw-Hill. Pfeffermann, Danny. 2011. “Modelling of Complex Survey Data: Why Model? Why Is It a Problem? How Can We Approach It?” Survey Methodology 37 (2): 115–36. Shah, B. V., M. M. Holt, and R. F. Folsom. 1977. “Inference about Regression Models from Sample Survey Data.” Bulletin of the International Statistical Institute 41 (3): 43–57. Skinner, Chris J, Daniell Holt, and Tom M F Smith. 1989. Analysis of Complex Surveys. New York: John Wiley; Sons. Wolter, Kirk M. 2007. Introduction to Variance Estimation. 2nd ed. Statistics for Social and Behavioral Sciences. Springer. "],["logistic-models.html", "5.4 Logistic models", " 5.4 Logistic models To model the probability of discrete variables, we apply the principles of design-based inference. 5.4.1 Logistic Regression Model for Proportions Logistic regression is a regression method that allows the estimation of the probability of success for a binary qualitative variable based on other continuous or discrete covariates. The variable of interest is binary or dichotomous, meaning it takes a value of one (1) if the condition being observed is met and zero (0) otherwise. In this way, the observations are separated into groups according to the value taken by the predictor variable. If a categorical variable with two possible levels is coded as ones (1) and zeros (0), it is mathematically possible to fit a linear regression model \\(\\beta_0 + \\beta_1 x\\) using estimation techniques such as least squares. However, a problem arises with this approach: since the model is a straight line, it can produce estimated values that are less than zero or greater than one, which contradicts the theory requiring probabilities to always fall within the [0,1] range. The objective of logistic regression is to model the logarithm of the probability of belonging to each group. As a result, assignment is ultimately made based on the obtained probabilities. Logistic regression is ideal for modeling the probability of an event occurring as a function of various factors. Therefore, the approximate probability of the event is represented by a logistic function of the form: \\[ \\pi(\\textbf{x})= Pr(y = 1 | \\textbf{x}) = \\frac{\\exp\\{\\textbf{x}&#39;\\boldsymbol{\\beta}\\}}{1+\\exp\\{\\textbf{x}&#39;\\boldsymbol{\\beta}\\}} \\] It is important to note that linear regression should not be used when the dependent variable is binary, as it cannot directly estimate the probability of the studied event. Instead, logistic regression is used, where a transformation (logit) is applied to obtain the probability estimates of the studied event. Applying the logit function to both sides yields an expression similar to that of linear regression: \\[ g(\\textbf{x})=logit(\\pi(\\textbf{x}))=ln \\left\\{ \\frac{\\pi(\\textbf{x})}{1-\\pi(\\textbf{x})} \\right \\}= \\textbf{x}&#39;\\boldsymbol{\\beta} \\] Thus, a linear relationship is assumed between each of the explanatory variables and the logit of the response variable. There are at least three major differences between logistic regression and linear regression. First, in logistic regression, there is no requirement for a linear relationship between the explanatory variables and the variable of interest; second, the residuals of the model do not need to follow a normal distribution; and third, the residuals do not need to have constant variance (homoscedasticity). Using appropriate techniques that include complex sampling designs in inference, the estimated probability that the variable of interest takes a value of one, which is also the expected value of the variable of interest in a logistic regression model, is: \\[ \\hat{\\pi}(\\textbf{x})= \\frac{\\exp\\{\\textbf{x}&#39;\\hat{\\boldsymbol{\\beta}}\\}}{1+\\exp\\{\\textbf{x}&#39;\\hat{\\boldsymbol{\\beta}\\}}} \\] The variance of the estimated parameters is calculated using the following expression: \\[ var\\left(\\boldsymbol{\\hat{B}}\\right)=\\boldsymbol{J}^{-1}var\\left(S\\left(\\hat{\\boldsymbol{B}}\\right)\\right)\\boldsymbol{J}^{-1} \\] Where: \\[ S\\left(B\\right)=\\sum_{h}\\sum_{a}\\sum_{i}w_{hai}\\boldsymbol{D}_{hai}^{t}\\left[\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\left(1-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)\\right]^{-1}\\left(y_{hai}-\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)=0 \\] and, \\[ D_{hai} = \\frac{\\delta\\left(\\pi_{hai}\\left(\\boldsymbol{B}\\right)\\right)}{\\delta B_{j}} \\] Where \\(j=0,\\dots,p\\). Since the model uses a logarithmic link, confidence intervals are constructed by applying the exponential function to each parameter: \\[ \\hat{\\psi}=\\exp\\left(\\hat{B}_{1}\\right) \\] Therefore, the confidence interval is given by the following expression: \\[ CI\\left(\\psi\\right)=\\exp\\left(\\hat{B}_{j}\\pm t_{df,1-\\frac{\\alpha}{2}}se\\left(\\hat{B}_{j}\\right)\\right) \\] It is important to note that interpreting logistic regression coefficients can be challenging due to its non-linear nature. To facilitate interpretation, similarities and key differences with classic linear regression are highlighted. One similarity is that the sign of the estimated equation can be interpreted in the same way in both cases. A positive sign accompanying a covariate indicates an increase in the probability of the event occurring, while a negative sign indicates a decrease. As in linear regression, the intercept can only be interpreted assuming zero values for the other predictors. However, the interpretation of regression coefficients between logistic and linear models differs significantly. The estimated coefficients in logistic regression correspond to a logarithm of odds, requiring the previously mentioned transformation. According to Gelman and Hill (2019), the exponentiated logistic regression coefficients can be interpreted as odds ratios. If two outcomes present probabilities of \\((\\pi,1-\\pi)\\), then \\(\\pi/(1-\\pi)\\) is called the odds. For example, an odds ratio of 1 corresponds to a probability of 0.5, indicating equally likely outcomes. Doubling the odds further increases the probability to 8/9, and so on. To determine whether a variable is significant in the model, it is common to use the Wald statistic, which is based on the likelihood ratio. In this case, the full model (with all parameters) is compared to the reduced model (with fewer parameters). The test statistic is given by: \\[ G=-2\\ln\\left[\\frac{L\\left(\\hat{\\boldsymbol{\\beta}}\\right)_{reduced}}{L\\left(\\hat{\\boldsymbol{\\beta}}\\right)_{full}}\\right] \\] This statistic evaluates the difference in fit between the two models, allowing for the assessment of the significance of the parameters included in the full model. References Gelman, Andrew, and Jennifer Hill. 2019. Data Analysis Using Regression and Multilevel/Hierarchical Models. Third Edition. Cambridge, UK: Cambridge University Press. "],["nso-practical-example-2.html", "5.5 NSO – Practical example", " 5.5 NSO – Practical example In this subsection, we will share the experience of an NSO in applying models to household survey data, and the results they present in terms of significance of models and relations among variables. "],["tables.html", "6 Tables", " 6 Tables Tables form a key component regarding how agencies disseminate statistics from household survey data. Using tables efficiently helps minimize the amount of numeric values presented within the text, and to organise the survey results for presentation to the users and wider audiences. It is therefore important to discuss some core principles and ideas to the preparation and production of tables with survey results. Before we enter detailed discussions, it is important to distinguish three main types of tables that can be used for presenting the results of a survey: presentation tables; reference tables; long / database like tables. The guidelines for each of these kinds of tables will be somewhat different, though all three types should follow some key principles, as suggested by Miller (2004), namely: Principle 1. Make it easy for your reader to find and understand the numbers presented in your tables; Principle 2. Draw the layout and labels of the table in a simple and direct way, helping to focus attention on the results you want to show. "],["presentation-tables.html", "6.1 Presentation tables", " 6.1 Presentation tables These are generally small tables, used to highlight certain key results obtained from the survey, to be presented in press releases, executive summaries, scientific articles or reports, or on landing web pages which contain the survey output. They are not expected to provide all results on a topic, but rather to highlight key results that should draw the attention of a reader to some of the main stories the data have produced. In presentation tables, the data should be presented concisely, and organized to support the text with the analysis of the corresponding data. They should be designed in such a way to help readers learn about the key results on the topic provided by the survey. Short, well-designed and formatted tables can provide a lot of information that readers can absorb quickly. This applies to tables published in any vehicle: reports, press releases, articles, electronic publications or websites. The example below illustrates the idea. Presentation tables should have rows (and possibly columns) sorted in a way that helps the reader perceive patterns, such as high or low figures. Such tables will often sacrifice detail in exchange for readability and understanding. Numbers should be presented with no more than 3 or 4 digits altoghether. If they area population counts, use thousands. If the figures are percentages, use no more than a single decimal digit, or even present only percentages rounded to the nearest integer, if the precision of the estimates do not warrant providing decimals (e.g. margins of error larger than 1%). "],["example-of-presentation-table-and-corresponding-text---include-in-a-box..html", "6.2 Example of presentation table and corresponding text - include in a box.", " 6.2 Example of presentation table and corresponding text - include in a box. Among middle and senior managers, women are outnumbered at all ages. The underrepresentation of women was observed in all age groups. Relative to their share among non-managers, women were outnumbered among middle and senior managers. In all age groups, women accounted for about 4 in 10 middle managers and 3 in 10 senior managers. Table 2 - Share of women (%) by age group and occupation Age group Non-managers Middle managers Senior managers 25 to 34 years 44.6 40.3 28.4 35 to 44 years 45.7 38.7 31.3 45 to 54 years 48.3 40.5 31.7 Note: The category “women” includes women, as well as some non-binary people. Source: Statistics Canada, Census of Population, 2021. https://www150.statcan.gc.ca/n1/pub/36-28-0001/2024010/article/00005-eng.htm "],["reference-tables.html", "6.3 Reference tables", " 6.3 Reference tables These are longer tables, generally used to present more complete sets of results from statistical studies. They should be limited in size to something that could be contained in a few pages of a printed report, say, with a number of rows no larger than 200, and no more than say 12 columns. Anything bigger than that should be considered for dissemination as a database like table, probably available only for download from a website or readable from some digital media. Reference tableswill typically take core classification, domain definition or explanatory variables to define the rows, and have the outcome classification or output variables define the columns. In both directions, sorting should typically be such that it is easier for the readers to locate the data that they are most interested in, either using alphabetic or well known classifications. Reference tables have in many cases been replaced by access to interactive databases that allow the interested user to obtain the tables they want from a website. Tables (of all types) should be self-sustaining. The idea is that each table should have the necessary metadata, so that if copied from one location to another it still makes sense. If you can get your tables to be self-sustaining, they will be easier to understand correctly, either in or out of the original context. Anatomy of a table. Figure XX presents the essential components of a table. The title (and optional subtitle) of a table must provide a clear and precise indication of the data that will be presented in the table. These elements, combined, must answer the questions about what, where and when regarding the data to be presented inside the table. Be concise and avoid using verbs. Column header elements should identify the data that is displayed in each column of the table. They must also provide much of the relevant metadata: unit of measurement, time period, geographical area, etc. Stub elements, provided as the first column in the table, should identify the data that is displayed in each row of the table. The source of the data must always be provided at the bottom of the table, and must indicate the organization responsible and the name of the survey or study that produced the results contained in the table. The omission of the citation of the source prevents the reader from seeking more information about the data presented, and should be avoided. The Notes are optional, but they can be used to provide additional details about the data as needed to understand and use it correctly. Avoid using long texts, which if needed, would be better placed in a document that is then cited in the Notes section. If there is more than one Note, number sequentially, and use the numbers to indicate the corresponding calls inside the table. Make sure that the calls to Notes are sufficiently distinct from the actual figures / numbers inside te table to avoid confusion. The Data is the most important piece of information that the user expects to get from the table. Therefore, it is essential to present them in a way that is easy to extract the relevant information. For some tables, depending on the message you want to convey, it may be easier to search for information by rows or columns. This should be the most important consideration when deciding whether to present the table in portrait or landscape orientation. Dividing lines, dotted lines, shading, and even spacing can be helpful in guiding the reader to read the table in the ‘right’ direction. Some basic rules for presenting the data include: Use similar spacing for columns whenever possible; Avoid any unnecessary text; The width of the table should be only the width necessary to present the data, and not the entire width of the available space; Time series data should be presented in chronological order – for reference tables, in ascending order; for presentation tables, this order can be reverse or descending to display the most recent data first; Data on categorical variables should be presented using standard classifications; in reference tables, categories should be ordered according to the standard classification; in presentation tables, they can be presented in (descending) order of frequency to highlight the most frequent categories first; Use as few decimal places as possible; Use thousands separators; space is a better separator because it does not vary with the decimal separator according to language; Always align the numbers to the right, ensuring that the decimal separator (comma or period, depending on the language) are aligned; Never center the values unless they all have the same number of digits; Do not leave blank cells on the table; missing values or ‘not applicable’ situations must be identified with an appropriate symbol; Round the data to units that make sense in each case; aim for providing 3 or 4 significant digits in presentation tables; Rounding is also useful when the data is not very accurate, but be careful not to lose precision. The recommendations provided here to reference tables should also apply to longer tables provided as databases, but these can have additional resources if they are embedded on websites. For example, there may be support for users to sort tables using the values in each column, which would be useful for large tables where the user may be looking for the higher (or lower) values in a given column. "],["data-visualization.html", "7 Data visualization", " 7 Data visualization In this section we discuss how to present data and estimates resulting from household surveys using graphics. While standard plots can still be used to show distributions and associations from the raw (unweighted) sample data, these can be misleading for the corresponding population distributions and associations. Therefore it is recommended that modified plots that account for survey weights be used instead. In addition, regarding the display of survey estimates, which are subject to sampling error, it is important to convey this message by presenting not only point estimates, but also standard errors or confidence intervals. Graphs are important for the exploratory analysis of the survey data, for the diagnostics of fitted models and for the presentation of results. However, often the sample data sets are very large. In addition, sampling units typically have different weights. These two difficulties may cause standard graphs to fail in facilitating the analysis or presentation. "],["graphs-and-sampling-weights.html", "7.1 Graphs and sampling weights", " 7.1 Graphs and sampling weights When the survey units have different sampling weights, these should be taken into account when preparing graphs with their data. The main reason is that weights can be interpreted as the number of population units that each sample unit represents. Hence, it is evident that unequal weights need to be considered in the elaboration of graphs based on such sample data. "],["graphs-for-categorical-data.html", "7.2 Graphs for categorical data", " 7.2 Graphs for categorical data When the data of interest are categorical, their descriptive analysis will be done using contingency tables. Then simple graphs like bar charts can be done using as input contingency tables produced with weighted counts. Ideally one should also aim to display error lines overlaying bars to indicate their respective confidence interval widths, thus conveying the uncertainty of the corresponding point estimates. Obtaining the weighted counts or proportions and their confidence intervals can be easily done using tools from several software packages, e.g. the survey package in R. As an example, the bar chart presents a comparison of the number of individuals (Nd) between rural and urban zones, with error lines indicating the confidence intervals for each estimate. According to the values in the table, the urban zone shows a slightly higher Nd value than the rural zone, with 78,164 individuals in the urban area compared to 72,102 in the rural area. This difference suggests a higher concentration of people in the urban zone. The confidence intervals allow us to assess the precision of these estimates. In the rural zone, the confidence interval ranges from 66,039 to 78,165 individuals, while in the urban zone, the confidence range goes from 72,526 to 83,802 individuals. This overlap between the intervals indicates that, although the urban zone has a higher number of individuals, the difference is not pronounced enough to be statistically significant. Furthermore, the standard deviation of Nd is 3,062 for the rural zone and 2,847 for the urban zone, reflecting similar variability in both zones. This suggests that the estimates are consistent in terms of relative uncertainty, without major differences in data dispersion between the zones. Figure 7.1: Distribution of Population Income Population distribution by area Zone Number of Individuals (Nd) Standard Error (Nd_se) Lower Limit (Nd_low) Upper Limit (Nd_upp) Rural 72,102 3,062 66,039 78,165 Urban 78,164 2,847 72,526 83,802 "],["histograms.html", "7.3 Histograms", " 7.3 Histograms Histograms serve to present the distribution of a single numeric (continuous) survey variable or response. If one had a census, then the histogram is a powerful tool to describe the underlying distribution, even for very large datasets. When displaying sample data, however, the sampling weights must be taken into account when estimating frequencies or relative frequencies of population units having values in the specified histogram bins. Modern survey analysis tools can easily provide weighted histograms where the sampling weights are incorporated. Histograms are the precursors to density function estimates, and the later can be thought of as histograms with very large number of bins. The survey package in R provides functions that can plot smoothed density estimates obtained accounting for the sampling weights. 7.3.1 Graphical Analysis with Survey Tools Once the database containing the sample is available and the sampling design has been defined, initial visual analyses can be conducted. It is recommended to begin with graphical analyses that, thanks to the principle of representativeness, reflect the behavior of continuous variables not only in the obtained sample but also in the study population, using sampling weights for sample expansion. A common example of visualization in this type of analysis is the use of histograms to represent the distribution of variables such as income. These charts allow us to observe the distribution of the variable of interest in the expanded population and to understand its shape, dispersion, and general trends. It is also common to perform graphical analyses broken down by subgroups, such as geographic areas (urban and rural) or thematic characteristics like gender (male and female). This approach helps identify key differences among specific population subgroups, for instance, by examining income distribution in men and women over the age of 18. Such breakdowns help visualize and communicate potential gaps between subgroups of interest. In this way, charts help to communicate results in an accessible manner, offering a clear and straightforward visual representation for audiences who may not be familiar with the technical details of estimation methods. In 7.2 the horizontal axis (x) represents income levels, spanning from 0 to over 4000 monetary units, while the vertical axis (y) indicates frequency, meaning the number of individuals within each income range. The distribution shows that most of the population is concentrated at lower income levels, with a particularly high frequency near 0. As income levels rise, frequency declines sharply, indicating a right-skewed (positively skewed) distribution with a smaller proportion of people at higher income levels. The light gray bars visually emphasize this concentration at lower incomes, highlighting a significant disparity in the population’s income distribution. Figure 7.2: Distribution of Population Income As an example, Figure 7.3 presents two histograms illustrating the distribution of income and expenditure by sex. In the histogram on the left, titled “Income Histograms by Sex,” we observe the income distribution, where blue bars represent men and pink bars represent women. The majority of the population, both male and female, is concentrated in the lower income levels, showing a right-skewed distribution. In the lower income levels, there are more men than women, while at higher income levels, the difference is less pronounced. In the histogram on the right, titled “Expenditure Histograms by Sex,” the distribution of expenditure is shown, also broken down by sex. Similar to income, most of the population of both sexes is concentrated in the lower expenditure levels, with a right-skewed trend. There is also a higher proportion of men in the lower expenditure levels, while at higher levels, the representation between sexes is more balanced. These histograms exemplify the similarity in the income and expenditure distributions between men and women, although men appear to be slightly more represented in the lower levels of both variables. Figure 7.3: Histograms of Income and Expenditure by Sex "],["box-plots.html", "7.4 Box Plots", " 7.4 Box Plots Box plots are often used to present the distribution of continuous variables. They can summarize large datasets by providing a visual display providing easy visual to identify location, range, variability and outliers. They are great also for enabling comparing distributions across specified grouping variables, such as strata, clusters, etc. The key to producing such graphs from complex sample surveys is to account for the sampling weights when estimating the location measures that drive the plot, namely the quartiles. Once these have been estimated using the methods described earlier, the resulting box plots will be good depictions enabling analysis of the underlying population distributions. "],["scatter-plots.html", "7.5 Scatter Plots", " 7.5 Scatter Plots Scatter plots are the tool of choice to explore relationships between two continuous variables, potentially revealing patterns or trends in the data. These plots face the two challenges discussed above. First one needs to try and convey in the plot that the different sample observations carry different weights. For small to moderate sample sizes this can be done by plotting circles or dots of varying sizes where the symbol size represents the corresponding observation sampling weight. Plots like these can be obtained using standard bubble plot tools or the scatter plot available in the survey package in R. The second challenge, present when there is a large dataset to be displayed, has motivated creation of some alternatives. Two ideas are worth noting. The first one is subsampling. One may choose to select a small to moderate subsample from the full dataset to display. Such a sample should be selected with replacement and with probabilities proportional to the observations sampling weights. Then the resulting smaller dataset can be used to produce a standard scatter plot. The subsample obtained in this way behaves approximately as a simple random sample from the survey population - see Lumley (2010) page 69. The other alternative is to produce so-called hexagonal binned scatter plots. This involves dividing the plot surface into a grid of hexagons and combining all the points that fall into a grid cell into a single plotted hexagon whose shading or size indicates the number of points in the bin. With complex household survey data, the number of points in a hexagonal bin should be replaced by the sum of the weights for points in the bin - see Lumley (2010) page 70. The third alternative is to avoid the display of the individual data points altogether, but instead produce smoothed scatter plots. One idea that can be useful would be to estimate specified quantiles (say the quartiles) of the y-axis (response) variable conditional on the values of the x-axis (predictor) variable, and smooth these across the range of the x-axis. Such plots can easily signal whether the y-variable has any relationship with the x-variable, and suggest the kind of curves that might be useful in summarising or modelling such a relationship - see Lumley (2010) page 71. As an example in 7.4, the following scatterplot is presented, showing the weighted relationship between income and expenditure in a population. In this plot, the size of the points represents the weight assigned to each observation. A high concentration of points is observed at lower income and expenditure levels, suggesting that most of the population has low income and low expenditure. Although there is an upward trend, indicating that income and expenditure tend to increase together, the dispersion of points reveals that higher expenditure is not always associated with proportionally higher income. Some larger points, corresponding to observations with greater weight, are distributed across different levels of income and expenditure without concentrating in a single area. Additionally, a few isolated points at high expenditure levels may represent outliers with considerably higher-than-average expenditure. Overall, this plot suggests a positive relationship between income and expenditure, accompanied by significant variability and some exceptional cases. Figure 7.4: Weighted scatterplot between income and expenditure References Lumley, Thomas. 2010. Complex Surveys: A Guide to Analysis Using r. Wiley Series in Survey Methodology. John Wiley; Sons. "],["maps.html", "7.6 Maps", " 7.6 Maps Maps are the display of choice to present the behavior of the interest variable across geographical domains. Maps that aim to present how a single response variable behaves can be easily obtained by plotting a summary of the response across the domains. Such a summary (say mean or median) should be an estimate for the corresponding population parameter btained accounting for the sample design and weights. Secondary survey analysts will may find that the limits of what mapping they can do is the level of geographic detail provided with the survey microdata. Many household sample surveys are design to provide precise estimates at some broad geographic level, say the country or its first level geographic subdivisions, such as states or departments. Lower level geographies are seldom disseminated with the survey microdata due to confidentiality protection constraints imposed. It is therefore important that statistical agencies conducting the household sample surveys and preparing the dissemination of the corresponding microdata consider carefully which level of geographic detail may be included with public use datasets. One area which still needs further research is that of providing appropriate means to convey the uncertainty of underlying point estimates when mapping these. "],["nso-practical-example-3.html", "7.7 NSO – Practical example", " 7.7 NSO – Practical example In this subsection we will include the experience of a NSO on displaying information through graphics. "],["references.html", "References", " References Binder, David A. 1983a. “On the Variances of Asymptotically Normal Estimators from Complex Surveys.” International Statistical Review 51: 279–92. Binder, David A. 1983b. “On the Variances of Asymptotically Normal Estimators from Complex Surveys.” International Statistical Review 51 (3): 279–92. https://doi.org/10.2307/1402588. Binder, David A., and Milojica S. Kovacevic. 1995. “Estimating Some Measures of Income Inequality from Survey Data: An Application of the Estimating Equations Approach.” Survey Methodology 21 (2): 137–45. Dean, Natalie, and Marcello Pagano. 2015. “Evaluating Confidence Interval Methods for Binomial Proportions in Clustered Surveys.” Journal of Survey Statistics and Methodology 3 (4): 484–503. https://doi.org/10.1093/jssam/smv024. Efron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. Fay, R. E. 1979. “On Adjusting the Pearson Chi-Square Statistic for Clustered Sampling.” ASA Proceedings of the Social Statistics Section, 402–8. Fellegi, Ivan P. 1980. “Approximate Joint Estimation of the Parameters of Multinomial Distributions in the Analysis of Data from Complex Surveys.” Journal of the American Statistical Association 75 (370): 261–68. Fuller, Wayne A. 1975. “Regression Analysis for Sample Survey.” Sankyha, Series C 37: 117–32. ———. 2002. “Regression Estimation for Survey Samples (with Discussion).” Survey Methodology 28 (1): 5–23. Gelman, Andrew, and Jennifer Hill. 2019. Data Analysis Using Regression and Multilevel/Hierarchical Models. Third Edition. Cambridge, UK: Cambridge University Press. Gutiérrez, H. A. 2016. Estrategias de Muestreo: Diseño de Encuestas y Estimación de Parámetros. Segunda edición. Ediciones de la U. Gutiérrez, Hugo Andrés. 2015. TeachingSampling: Selection of Samples and Parameter Estimation in Finite Population. https://CRAN.R-project.org/package=TeachingSampling. Hansen, Morris H., William N. Hurwitz, and William G. Madow. 1953. Sample Survey Methods and Theory. Vol. 1 and 2. New York: John Wiley; Sons. Heeringa, Steven G., Brady T. West, and Patricia A. Berglund. 2017a. Applied Survey Data Analysis. Chapman and Hall CRC Statistics in the Social and Behavioral Sciences Series. CRC Press. Heeringa, Steven G, Brady T West, and Patricia A Berglund. 2017b. Applied Survey Data Analysis, Second Edition. Statistics in the Social and Behavioral Sciences. 2nd edition. Chapman; Hall - CRC. IBM. 2017. IBM SPSS Complex Samples. ftp://public.dhe.ibm.com/software/analytics/spss/documentation/statistics/23.0/en/client/Manuals/IBM_SPSS_Complex_Samples.pdf. Kish, Leslie, and Martin R Frankel. 1974. “Inference from Complex Samples.” Journal of the Royal Statistical Society, Series B 36: 1–37. Kovar, J. G., J. N. K. Rao, and C. F. J. Wu. 1988. “Bootstrap and Other Methods to Measure Errors in Survey Estimates.” Canadian Journal of Statistics 16 (Suppl.): 25–45. Langel, Matti, and Yves Tillé. 2013. “Variance Estimation of the Gini Index: Revisiting a Result Several Times Published: Variance Estimation of the Gini Index.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 176 (2): 521–40. https://doi.org/10.1111/j.1467-985X.2012.01048.x. Lumley, Thomas. 2010. Complex Surveys: A Guide to Analysis Using r. Wiley Series in Survey Methodology. John Wiley; Sons. ———. 2016. “Survey: Analysis of Complex Survey Samples.” Nations, United. 2005. Household Surveys in Developing and Transition Countries. New York, NY: United Nations. Neter, John, William Wasserman, and Michael H. Kutner. 1996. Applied Linear Statistical Models. McGraw-Hill. Osier, Guillaume. 2009. “Variance Estimation for Complex Indicators of Poverty and Inequality.” Journal of the European Survey Research Association 3 (3): 167–95. http://ojs.ub.uni-konstanz.de/srm/article/view/369. Pfeffermann, Danny. 2011. “Modelling of Complex Survey Data: Why Model? Why Is It a Problem? How Can We Approach It?” Survey Methodology 37 (2): 115–36. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rao, J. N. K., and A. J. Scott. 1984. “On Chi-Squared Tests for Multiway Contingency Tables with Cell Proportions Estimated from Survey Data.” The Annals of Statistics 12: 46–60. Rao, J. N. K., C F J Wu, and K. Yue. 1992. “Some Recent Work on Resampling Methods for Complex Surveys.” Survey Methodology 18: 209–17. Rojas, Hugo Andres Gutierrez. 2020. Samplesize4surveys: Sample Size Calculations for Complex Surveys. Rust, Keith F., Valerie Hsu, and Westat. 2007. “Confidence Intervals for Statistics for Categorical Variables from Complex Samples.” In. https://api.semanticscholar.org/CorpusID:195852485. Särndal, Carl-Erik, Bengt Swensson, and Jan Wretman. 1992. Model Assisted Survey Sampling. New York: Springer-Verlag. SAS. 2010. SAS/STAT 9.22 User’s Guide - Survey Sampling and Analysis Procedures. https://support.sas.com/documentation/cdl/en/statugsurveysamp/63778/PDF/default/statugsurveysamp.pdf. Shah, B. V., M. M. Holt, and R. F. Folsom. 1977. “Inference about Regression Models from Sample Survey Data.” Bulletin of the International Statistical Institute 41 (3): 43–57. Shah, Babubhai. V, Ralph E Folsom, Lisa LaVange, Sara C Wheeless, Kerrie E Boyle, and Rick L Williams. 1993. “Statistical Methods and Mathematical Algorithms Used in SUDAAN.” Research Triangle Institute. Skinner, Chris J, Daniell Holt, and Tom M F Smith. 1989. Analysis of Complex Surveys. New York: John Wiley; Sons. STATA. 2017. STATA Survey Data. https://www.stata.com/manuals13/svy.pdf. Thomas, D. R., and J. N. K. Rao. 1987. “Small-Sample Comparisons of Level and Power for Simple Goodness-of-Fit Statistics Under Cluster Sampling.” Journal of the American Statistical Association 82: 630–36. Tillé, Yves, and Alina Matei. 2016. Sampling: Survey Sampling. https://CRAN.R-project.org/package=sampling. Westat. 2007. WesVar 4.3. Users Guide. http://users.nber.org/~jroth/chap1.pdf. Wolter, Kirk M. 2007. Introduction to Variance Estimation. 2nd ed. Statistics for Social and Behavioral Sciences. Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
